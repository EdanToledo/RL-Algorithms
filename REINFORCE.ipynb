{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# REINFORCE - Monte Carlo Policy Gradient \n",
    "\n",
    "REINFORCE is most likely the simplest policy gradient method. It uses Monte Carlo returns as a direct estimate for the policy's Q value. \n",
    "\n",
    "The policy gradient is:\n",
    "$\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta)\n",
    "&= \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)] & \\\\\n",
    "&= \\mathbb{E}_\\pi [G_t \\nabla_\\theta \\ln \\pi_\\theta(A_t \\vert S_t)] & \\scriptstyle{\\text{; Because } Q^\\pi(S_t, A_t) = \\mathbb{E}_\\pi[G_t \\vert S_t, A_t]}\n",
    "\\end{aligned}$\n",
    "\n",
    "We can use the Monte Carlo return of Gt (the discounted value of future rewards) in place of the policy's Q value in the gradient update.\n",
    "\n",
    "The algorithm is very simple compared to DQN and with much less moving parts.\n",
    "\n",
    "Below is the pseudocode simplest case of episodic REINFORCE:\n",
    "\n",
    "![Pseudocode](https://miro.medium.com/max/4800/1*NQkoA-eQOUXHqIln-WzMpw.png)\n",
    "\n",
    "The following is a PyTorch implementation of the REINFORCE algorithm for both discrete and continuous action spaces."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discrete Policy\n",
    "The discete action space policy is simply a neural network with the following layer sizes:\n",
    "- Input : State Observation size\n",
    "- Hidden : Whatever you please.\n",
    "- Output: Number of possible actions.\n",
    "\n",
    "The policy then uses a softmax activation function at the end to give probabilities for selecting an action."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Discrete_Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Discrete_Policy, self).__init__()\n",
    "        self.policy_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions),\n",
    "                                        nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.policy_net(obs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Continuous Policy\n",
    "The continuous action space policy is a little more complicated. This can be done differently depending on what you want to do but the following uses two separate neural networks to approximate a mean and std. deviation respectively.\n",
    "\n",
    "The sizes of both neural networks are as follows:\n",
    "\n",
    "- Input : State Observation size\n",
    "- Hidden : Whatever you please.\n",
    "- Output: 1\n",
    "\n",
    "One doesnt need to use a separate neural network for the std deviation, they can use a single learned parameter or even some fixed constant but all will have different results and this involves some experimentation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Continuous_Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Continuous_Policy, self).__init__()\n",
    "        self.mean_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions))\n",
    "        self.standard_deviation_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.mean_net(obs),torch.abs(self.standard_deviation_net(obs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent \n",
    "The following is the agent class:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "class REINFORCE_Agent:\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,nb_actions,discrete=True,discount_factor=0.9,learning_rate=0.0001):\n",
    "        self.discrete = discrete\n",
    "        if discrete:\n",
    "            self.policy = Discrete_Policy(input_size,hidden_size,nb_actions)\n",
    "        else:\n",
    "            self.policy = Continuous_Policy(input_size,hidden_size,nb_actions)\n",
    "        \n",
    "        self.memory = []\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        self.optimizer = AdamW(self.policy.parameters(),learning_rate)\n",
    "\n",
    "        self.best_weights = self.policy.state_dict()\n",
    "        \n",
    "    def act(self,obs):\n",
    "        if self.discrete:\n",
    "            # Create categorical distribution using the action probabilities given by the agent's policy\n",
    "            action_dist = Categorical(self.policy(obs))\n",
    "\n",
    "        else:\n",
    "            mu,dev = self.policy(obs)\n",
    "            # Create a Normal distribution using the policy given mean and std deviation\n",
    "            action_dist = Normal(mu,dev)\n",
    "        \n",
    "        # Return the action distribution created according to policy\n",
    "        return action_dist\n",
    "\n",
    "    def cache(self,reward,log_action_prob):\n",
    "        \"\"\"\n",
    "        Add reward and the log of the performed action's probability\n",
    "        \"\"\"\n",
    "        self.memory.append((reward,log_action_prob))\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.memory.clear()\n",
    "    \n",
    "    def train(self):\n",
    "        self.policy.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.policy.load_state_dict(self.best_weights)\n",
    "        self.policy.eval()\n",
    "        \n",
    "    def update_model(self):\n",
    "        \n",
    "        returns_to_go = []\n",
    "        # Calculate the Monte Carlo estimates i.e (sum of discounted future rewards for each timestep)\n",
    "        Gt=0\n",
    "        for (reward,_) in self.memory[::-1]:\n",
    "            Gt = reward+self.gamma*Gt\n",
    "            returns_to_go.append(Gt)\n",
    "\n",
    "        returns_to_go = returns_to_go[::-1]\n",
    "\n",
    "        # Sum every transitions loss: the policy gradient as stated above is:\n",
    "        # ∇J = E[ Gt * ∇log(π(a|s))]\n",
    "        # PyTorch calculates the gradient of the loss to use in the policies parameter updates\n",
    "        # therefore we only need to make the loss = Gt * log(action probability)\n",
    "        # REINFORCE assumes gradient ascent so we make it negative to work with pytorch gradient descent\n",
    "        losses = []\n",
    "        for Gt,(reward,log_action_prob) in zip(returns_to_go,self.memory):\n",
    "            losses.append(-log_action_prob*Gt)\n",
    "        \n",
    "        # clear the optimizers current gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # sum the losses\n",
    "        loss = torch.stack(losses).sum()\n",
    "\n",
    "        # backpropagate the loss to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # make one optimization step\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # clear the episode memory from the agents memory\n",
    "        self.clear_memory()\n",
    "    \n",
    "    def update_best_weights(self):\n",
    "        self.best_weights = self.policy.state_dict()\n",
    "\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "The last thing needed is the training loop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "from numpy import double\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "def train(agent, env, num_episodes, num_steps, log_every=20, render_from=1000):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to train.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    :param render_from: The episode number to start rendering to screen to allow one to view agent. Rendering significantly slows down training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    best_running_avg = 0\n",
    "    agent.train()\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = env.reset()\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "            if episode>render_from:\n",
    "                env.render()\n",
    "            # Return agents action probability distribution\n",
    "            action_dist = agent.act(torch.tensor(obs,dtype=torch.float32))\n",
    "            # Sample an action from this distribution\n",
    "            action = action_dist.sample()\n",
    "            # Take a step in the environment with the action drawn\n",
    "            obs , reward, done, info = env.step(action.numpy())\n",
    "            reward_total+=reward\n",
    "            # Save the transition information needed for update\n",
    "            agent.cache(reward,action_dist.log_prob(action))\n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    if running_avg > best_running_avg:\n",
    "                        best_running_avg = running_avg\n",
    "                        agent.update_best_weights()\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        # Update model every episode\n",
    "        agent.update_model()\n",
    "        running_avg_reward.append(reward_total)\n",
    "        \n",
    "\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def eval(agent, env, num_episodes, num_steps, log_every=20):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to eval.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = env.reset()\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "           \n",
    "            env.render()\n",
    "            # Return agents action probability distribution\n",
    "            action_dist = agent.act(torch.tensor(obs,dtype=torch.float32))\n",
    "            # Sample an action from this distribution\n",
    "            action = action_dist.sample()\n",
    "            # Take a step in the environment with the action drawn\n",
    "            obs , reward, done, info = env.step(action.numpy())\n",
    "            reward_total+=reward\n",
    "           \n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time to Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "if (isinstance(env.action_space,gym.spaces.Discrete)):\n",
    "    discrete = True\n",
    "    nb_actions = env.action_space.n\n",
    "else:\n",
    "    discrete = False\n",
    "    nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Hyper parameters\n",
    "HIDDEN_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0005\n",
    "EPISODES_TO_TRAIN = 1000\n",
    "EPISODES_TO_EVAL = 20\n",
    "MAX_STEPS = 201\n",
    "LOG_EVERY = 20\n",
    "RENDER_FROM_EP = 2000\n",
    "\n",
    "agent = REINFORCE_Agent(env.observation_space.shape[0],HIDDEN_SIZE,nb_actions,discrete,GAMMA,LEARNING_RATE)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "train(agent,env,EPISODES_TO_TRAIN,MAX_STEPS,LOG_EVERY,RENDER_FROM_EP)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "\n",
    "eval(agent,env,EPISODES_TO_EVAL,MAX_STEPS,log_every=1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "05a25e9113b15a9f940c6517e4f0bb1aaafcc84783dc3502a8f30808da99fb6f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}