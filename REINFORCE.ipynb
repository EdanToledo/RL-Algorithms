{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# REINFORCE - Monte Carlo Policy Gradient \n",
    "\n",
    "REINFORCE is most likely the simplest policy gradient method. It uses Monte Carlo returns as a direct estimate for the policy's Q value. \n",
    "\n",
    "The policy gradient is:\n",
    "$\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta)\n",
    "&= \\mathbb{E}_\\pi [Q^\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)] & \\\\\n",
    "&= \\mathbb{E}_\\pi [G_t \\nabla_\\theta \\ln \\pi_\\theta(A_t \\vert S_t)] & \\scriptstyle{\\text{; Because } Q^\\pi(S_t, A_t) = \\mathbb{E}_\\pi[G_t \\vert S_t, A_t]}\n",
    "\\end{aligned}$\n",
    "\n",
    "We can use the Monte Carlo return of Gt (the discounted value of future rewards) in place of the policy's Q value in the gradient update.\n",
    "\n",
    "The algorithm is very simple compared to DQN and with much less moving parts.\n",
    "\n",
    "Below is the pseudocode simplest case of episodic REINFORCE:\n",
    "\n",
    "![Pseudocode](https://miro.medium.com/max/4800/1*NQkoA-eQOUXHqIln-WzMpw.png)\n",
    "\n",
    "The following is a PyTorch implementation of the REINFORCE algorithm for both discrete and continuous action spaces."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discrete Policy\n",
    "The discete action space policy is simply a neural network with the following layer sizes:\n",
    "- Input : State Observation size\n",
    "- Hidden : Whatever you please.\n",
    "- Output: Number of possible actions.\n",
    "\n",
    "The policy then uses a softmax activation function at the end to give probabilities for selecting an action."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Discrete_Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Discrete_Policy, self).__init__()\n",
    "        self.policy_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions),\n",
    "                                        nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.policy_net(obs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Continuous Policy\n",
    "The continuous action space policy is a little more complicated. This can be done differently depending on what you want to do but the following uses two separate neural networks to approximate a mean and std. deviation respectively.\n",
    "\n",
    "The sizes of both neural networks are as follows:\n",
    "\n",
    "- Input : State Observation size\n",
    "- Hidden : Whatever you please.\n",
    "- Output: 1\n",
    "\n",
    "One doesnt need to use a separate neural network for the std deviation, they can use a single learned parameter or even some fixed constant but all will have different results and this involves some experimentation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Continuous_Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Continuous_Policy, self).__init__()\n",
    "        self.mean_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions))\n",
    "        self.standard_deviation_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.mean_net(obs),torch.abs(self.standard_deviation_net(obs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent \n",
    "The following is the agent class:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "class REINFORCE_Agent:\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,nb_actions,discrete=True,discount_factor=0.9,learning_rate=0.0001):\n",
    "        self.discrete = discrete\n",
    "        if discrete:\n",
    "            self.policy = Discrete_Policy(input_size,hidden_size,nb_actions)\n",
    "        else:\n",
    "            self.policy = Continuous_Policy(input_size,hidden_size,nb_actions)\n",
    "        \n",
    "        self.memory = []\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        self.optimizer = AdamW(self.policy.parameters(),learning_rate)\n",
    "\n",
    "        self.best_weights = self.policy.state_dict()\n",
    "        \n",
    "    def act(self,obs):\n",
    "        if self.discrete:\n",
    "            # Create categorical distribution using the action probabilities given by the agent's policy\n",
    "            action_dist = Categorical(self.policy(obs))\n",
    "\n",
    "        else:\n",
    "            mu,dev = self.policy(obs)\n",
    "            # Create a Normal distribution using the policy given mean and std deviation\n",
    "            action_dist = Normal(mu,dev)\n",
    "        \n",
    "        # Return the action distribution created according to policy\n",
    "        return action_dist\n",
    "\n",
    "    def cache(self,reward,log_action_prob):\n",
    "        \"\"\"\n",
    "        Add reward and the log of the performed action's probability\n",
    "        \"\"\"\n",
    "        self.memory.append((reward,log_action_prob))\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.memory.clear()\n",
    "    \n",
    "    def train(self):\n",
    "        self.policy.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.policy.load_state_dict(self.best_weights)\n",
    "        self.policy.eval()\n",
    "        \n",
    "    def update_model(self):\n",
    "        \n",
    "        returns_to_go = []\n",
    "        # Calculate the Monte Carlo estimates i.e (sum of discounted future rewards for each timestep)\n",
    "        Gt=0\n",
    "        for (reward,_) in self.memory[::-1]:\n",
    "            Gt = reward+self.gamma*Gt\n",
    "            returns_to_go.append(Gt)\n",
    "\n",
    "        returns_to_go = returns_to_go[::-1]\n",
    "\n",
    "        # Sum every transitions loss: the policy gradient as stated above is:\n",
    "        # ∇J = E[ Gt * ∇log(π(a|s))]\n",
    "        # PyTorch calculates the gradient of the loss to use in the policies parameter updates\n",
    "        # therefore we only need to make the loss = Gt * log(action probability)\n",
    "        # REINFORCE assumes gradient ascent so we make it negative to work with pytorch gradient descent\n",
    "        losses = []\n",
    "        for Gt,(reward,log_action_prob) in zip(returns_to_go,self.memory):\n",
    "            losses.append(-log_action_prob*Gt)\n",
    "        \n",
    "        # clear the optimizers current gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # sum the losses\n",
    "        loss = torch.stack(losses).sum()\n",
    "\n",
    "        # backpropagate the loss to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # make one optimization step\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # clear the episode memory from the agents memory\n",
    "        self.clear_memory()\n",
    "    \n",
    "    def update_best_weights(self):\n",
    "        self.best_weights = self.policy.state_dict()\n",
    "\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "The last thing needed is the training loop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "from numpy import double\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "def train(agent, env, num_episodes, num_steps, log_every=20, render_from=1000):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to train.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    :param render_from: The episode number to start rendering to screen to allow one to view agent. Rendering significantly slows down training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    best_running_avg = 0\n",
    "    agent.train()\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = env.reset()\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "            if episode>render_from:\n",
    "                env.render()\n",
    "            # Return agents action probability distribution\n",
    "            action_dist = agent.act(torch.tensor(obs,dtype=torch.float32))\n",
    "            # Sample an action from this distribution\n",
    "            action = action_dist.sample()\n",
    "            # Take a step in the environment with the action drawn\n",
    "            obs , reward, done, info = env.step(action.numpy())\n",
    "            reward_total+=reward\n",
    "            # Save the transition information needed for update\n",
    "            agent.cache(reward,action_dist.log_prob(action))\n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    if running_avg > best_running_avg:\n",
    "                        best_running_avg = running_avg\n",
    "                        agent.update_best_weights()\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        # Update model every episode\n",
    "        agent.update_model()\n",
    "        running_avg_reward.append(reward_total)\n",
    "        \n",
    "\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "def eval(agent, env, num_episodes, num_steps, log_every=20):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to eval.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = env.reset()\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "           \n",
    "            env.render()\n",
    "            # Return agents action probability distribution\n",
    "            action_dist = agent.act(torch.tensor(obs,dtype=torch.float32))\n",
    "            # Sample an action from this distribution\n",
    "            action = action_dist.sample()\n",
    "            # Take a step in the environment with the action drawn\n",
    "            obs , reward, done, info = env.step(action.numpy())\n",
    "            reward_total+=reward\n",
    "           \n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time to Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "if (isinstance(env.action_space,gym.spaces.Discrete)):\n",
    "    discrete = True\n",
    "    nb_actions = env.action_space.n\n",
    "else:\n",
    "    discrete = False\n",
    "    nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Hyper parameters\n",
    "HIDDEN_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0005\n",
    "EPISODES_TO_TRAIN = 1000\n",
    "EPISODES_TO_EVAL = 20\n",
    "MAX_STEPS = 201\n",
    "LOG_EVERY = 20\n",
    "RENDER_FROM_EP = 2000\n",
    "\n",
    "agent = REINFORCE_Agent(env.observation_space.shape[0],HIDDEN_SIZE,nb_actions,discrete,GAMMA,LEARNING_RATE)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "train(agent,env,EPISODES_TO_TRAIN,MAX_STEPS,LOG_EVERY,RENDER_FROM_EP)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "\n",
    "eval(agent,env,EPISODES_TO_EVAL,MAX_STEPS,log_every=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training...\n",
      "Episode   20 finished after    6 timesteps with a total reward of 6.0 | Running Average: 6.32\n",
      "Episode   40 finished after    5 timesteps with a total reward of 5.0 | Running Average: 6.23\n",
      "Episode   60 finished after    4 timesteps with a total reward of 4.0 | Running Average: 6.02\n",
      "Episode   80 finished after    6 timesteps with a total reward of 6.0 | Running Average: 5.78\n",
      "Episode  100 finished after    5 timesteps with a total reward of 5.0 | Running Average: 5.75\n",
      "Episode  120 finished after    5 timesteps with a total reward of 5.0 | Running Average: 5.55\n",
      "Episode  140 finished after    5 timesteps with a total reward of 5.0 | Running Average: 5.28\n",
      "Episode  160 finished after    5 timesteps with a total reward of 5.0 | Running Average: 5.19\n",
      "Episode  180 finished after    8 timesteps with a total reward of 8.0 | Running Average: 5.35\n",
      "Episode  200 finished after    5 timesteps with a total reward of 5.0 | Running Average: 5.39\n",
      "Episode  220 finished after    5 timesteps with a total reward of 5.0 | Running Average: 5.48\n",
      "Episode  240 finished after    5 timesteps with a total reward of 5.0 | Running Average: 5.71\n",
      "Episode  260 finished after    6 timesteps with a total reward of 6.0 | Running Average: 5.84\n",
      "Episode  280 finished after    6 timesteps with a total reward of 6.0 | Running Average: 5.84\n",
      "Episode  300 finished after   16 timesteps with a total reward of 16.0 | Running Average: 6.19\n",
      "Episode  320 finished after    9 timesteps with a total reward of 9.0 | Running Average: 7.84\n",
      "Episode  340 finished after   31 timesteps with a total reward of 31.0 | Running Average: 10.18\n",
      "Episode  360 finished after   23 timesteps with a total reward of 23.0 | Running Average: 13.99\n",
      "Episode  380 finished after   18 timesteps with a total reward of 18.0 | Running Average: 17.95\n",
      "Episode  400 finished after   17 timesteps with a total reward of 17.0 | Running Average: 23.40\n",
      "Episode  420 finished after   28 timesteps with a total reward of 28.0 | Running Average: 26.54\n",
      "Episode  440 finished after   22 timesteps with a total reward of 22.0 | Running Average: 30.76\n",
      "Episode  460 finished after   51 timesteps with a total reward of 51.0 | Running Average: 31.92\n",
      "Episode  480 finished after   42 timesteps with a total reward of 42.0 | Running Average: 34.56\n",
      "Episode  500 finished after   15 timesteps with a total reward of 15.0 | Running Average: 33.53\n",
      "Episode  520 finished after   16 timesteps with a total reward of 16.0 | Running Average: 30.31\n",
      "Episode  540 finished after   10 timesteps with a total reward of 10.0 | Running Average: 25.10\n",
      "Episode  560 finished after    8 timesteps with a total reward of 8.0 | Running Average: 21.66\n",
      "Episode  580 finished after   27 timesteps with a total reward of 27.0 | Running Average: 15.74\n",
      "Episode  600 finished after   44 timesteps with a total reward of 44.0 | Running Average: 12.81\n",
      "Episode  620 finished after   12 timesteps with a total reward of 12.0 | Running Average: 14.27\n",
      "Episode  640 finished after   10 timesteps with a total reward of 10.0 | Running Average: 16.68\n",
      "Episode  660 finished after   33 timesteps with a total reward of 33.0 | Running Average: 18.97\n",
      "Episode  680 finished after   20 timesteps with a total reward of 20.0 | Running Average: 23.81\n",
      "Episode  700 finished after   48 timesteps with a total reward of 48.0 | Running Average: 27.95\n",
      "Episode  720 finished after   11 timesteps with a total reward of 11.0 | Running Average: 28.98\n",
      "Episode  740 finished after   44 timesteps with a total reward of 44.0 | Running Average: 27.02\n",
      "Episode  760 finished after   15 timesteps with a total reward of 15.0 | Running Average: 26.40\n",
      "Episode  780 finished after   37 timesteps with a total reward of 37.0 | Running Average: 23.14\n",
      "Episode  800 finished after   32 timesteps with a total reward of 32.0 | Running Average: 19.80\n",
      "Episode  820 finished after   27 timesteps with a total reward of 27.0 | Running Average: 19.32\n",
      "Episode  840 finished after   38 timesteps with a total reward of 38.0 | Running Average: 20.59\n",
      "Episode  860 finished after   37 timesteps with a total reward of 37.0 | Running Average: 21.57\n",
      "Episode  880 finished after   58 timesteps with a total reward of 58.0 | Running Average: 25.08\n",
      "Episode  900 finished after   36 timesteps with a total reward of 36.0 | Running Average: 28.84\n",
      "Episode  920 finished after   25 timesteps with a total reward of 25.0 | Running Average: 30.04\n",
      "Episode  940 finished after   53 timesteps with a total reward of 53.0 | Running Average: 33.02\n",
      "Episode  960 finished after   43 timesteps with a total reward of 43.0 | Running Average: 37.06\n",
      "Episode  980 finished after   53 timesteps with a total reward of 53.0 | Running Average: 40.32\n",
      "Episode 1000 finished after   36 timesteps with a total reward of 36.0 | Running Average: 45.89\n",
      "Evaluating...\n",
      "Creating window glfw\n",
      "Episode    2 finished after   39 timesteps with a total reward of 39.0 | Running Average: 43.00\n",
      "Episode    3 finished after   39 timesteps with a total reward of 39.0 | Running Average: 41.00\n",
      "Episode    4 finished after   65 timesteps with a total reward of 65.0 | Running Average: 40.33\n",
      "Episode    5 finished after   46 timesteps with a total reward of 46.0 | Running Average: 46.50\n",
      "Episode    6 finished after   10 timesteps with a total reward of 10.0 | Running Average: 46.40\n",
      "Episode    7 finished after   34 timesteps with a total reward of 34.0 | Running Average: 40.33\n",
      "Episode    8 finished after   79 timesteps with a total reward of 79.0 | Running Average: 39.43\n",
      "Episode    9 finished after   16 timesteps with a total reward of 16.0 | Running Average: 44.38\n",
      "Episode   10 finished after   37 timesteps with a total reward of 37.0 | Running Average: 41.22\n",
      "Episode   11 finished after   15 timesteps with a total reward of 15.0 | Running Average: 40.80\n",
      "Episode   12 finished after   12 timesteps with a total reward of 12.0 | Running Average: 38.45\n",
      "Episode   13 finished after   35 timesteps with a total reward of 35.0 | Running Average: 36.25\n",
      "Episode   14 finished after   40 timesteps with a total reward of 40.0 | Running Average: 36.15\n",
      "Episode   15 finished after   10 timesteps with a total reward of 10.0 | Running Average: 36.43\n",
      "Episode   16 finished after   39 timesteps with a total reward of 39.0 | Running Average: 34.67\n",
      "Episode   17 finished after   13 timesteps with a total reward of 13.0 | Running Average: 34.94\n",
      "Episode   18 finished after   17 timesteps with a total reward of 17.0 | Running Average: 33.65\n",
      "Episode   19 finished after   15 timesteps with a total reward of 15.0 | Running Average: 32.72\n",
      "Episode   20 finished after   39 timesteps with a total reward of 39.0 | Running Average: 31.79\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "05a25e9113b15a9f940c6517e4f0bb1aaafcc84783dc3502a8f30808da99fb6f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}