{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Actor-Critic\n",
    "\n",
    "The Actor-Critic framework can be seen as a merging of policy-based methods and value-based methods in the hopes of achieving the benefits of both. \n",
    "\n",
    "It is beneficial to learn the value function as well as the policy since knowing the value function can assist and push the policy updates in the correct direction and with less variance than Monte Carlo estimates. This also allows policy methods to be used in non-episodic environments as well as be updated more frequently as it doesn't need to wait until the end of the episode.\n",
    "\n",
    "Below is the Pseudocode for a one-step actor-critic:\n",
    "\n",
    "![Pseudocode](https://i.stack.imgur.com/zFfxs.png)\n",
    "\n",
    "The following will be an implementation of a one-step actor-critic but there are many other ways to do this. Instead of bootstrapping one could use Monte Carlo estimates to update the critic and update the policy on a whole episode of experience.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discrete Policy\n",
    "The discete action space policy is simply a neural network with the following layer sizes:\n",
    "- Input : State Observation size\n",
    "- Hidden : Whatever you please.\n",
    "- Output: Number of possible actions.\n",
    "\n",
    "The policy then uses a softmax activation function at the end to give probabilities for selecting an action."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Discrete_Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Discrete_Policy, self).__init__()\n",
    "        self.policy_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions),\n",
    "                                        nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.policy_net(obs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Continuous Policy\n",
    "The continuous action space policy is a little more complicated. This can be done differently depending on what you want to do but the following uses two separate neural networks to approximate a mean and std. deviation respectively.\n",
    "\n",
    "The sizes of both neural networks are as follows:\n",
    "\n",
    "- Input : State Observation size\n",
    "- Hidden : Whatever you please.\n",
    "- Output: 1\n",
    "\n",
    "One doesnt need to use a separate neural network for the std deviation, they can use a single learned parameter or even some fixed constant but all will have different results and this involves some experimentation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class Continuous_Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Continuous_Policy, self).__init__()\n",
    "        self.mean_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions))\n",
    "        self.standard_deviation_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.mean_net(obs),torch.abs(self.standard_deviation_net(obs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Critic\n",
    "This is the critic model - a neural network to learn the value function. This could be a Q (action-value) or V (state-value) approximator. This depends on implementation - here I am creating a state-value approximator."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        self.critic = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, 1))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.critic(obs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent \n",
    "The following is the agent class:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "class Actor_Critic_Agent:\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,nb_actions,discrete=True,discount_factor=0.9,learning_rate=0.0001):\n",
    "        self.discrete = discrete\n",
    "        if discrete:\n",
    "            self.policy = Discrete_Policy(input_size,hidden_size,nb_actions)\n",
    "        else:\n",
    "            self.policy = Continuous_Policy(input_size,hidden_size,nb_actions)\n",
    "        \n",
    "        self.critic = Critic(input_size,hidden_size)\n",
    "\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        # One can use one optimizer for both the critic and policy - this is a implementation detail\n",
    "        self.policy_optimizer = AdamW(self.policy.parameters(),learning_rate)\n",
    "        self.critic_optimizer = AdamW(self.critic.parameters(),learning_rate)\n",
    "        \n",
    "    def act(self,obs):\n",
    "        if self.discrete:\n",
    "            # Create categorical distribution using the action probabilities given by the agent's policy\n",
    "            action_dist = Categorical(self.policy(obs))\n",
    "\n",
    "        else:\n",
    "            mu,dev = self.policy(obs)\n",
    "            # Create a Normal distribution using the policy given mean and std deviation\n",
    "            action_dist = Normal(mu,dev)\n",
    "        \n",
    "        # Return the action distribution created according to policy\n",
    "        return action_dist\n",
    "    \n",
    "    def train(self):\n",
    "        self.policy.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.policy.eval()\n",
    "        \n",
    "    def update_model(self,state,reward,log_action_probability,next_state,done):\n",
    "        \n",
    "        # Current State Value\n",
    "        state_value = self.critic(state)\n",
    "\n",
    "        # Target State Value\n",
    "        target_value = reward+ (1-done)*self.gamma*self.critic(next_state)\n",
    "        \n",
    "        # Temporal Difference Error\n",
    "        td_error = target_value - state_value\n",
    "\n",
    "        # Critics Loss - Huber loss of state and target - can use other loss functions such as MSE\n",
    "        critic_loss = F.smooth_l1_loss(state_value,target_value)\n",
    "\n",
    "        # Policy Loss - log action probability shifted by the td_error\n",
    "        policy_loss = -td_error.detach()*log_action_probability\n",
    "\n",
    "        # clear the optimizers current gradients\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "\n",
    "        # backpropagate the loss to calculate the gradients\n",
    "        critic_loss.backward()\n",
    "        policy_loss.backward()\n",
    "\n",
    "        # make one optimization step\n",
    "        self.policy_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "The last thing needed is the training loop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "from numpy import double\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "def train(agent, env, num_episodes, num_steps, log_every=20, render_from=1000):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to train.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    :param render_from: The episode number to start rendering to screen to allow one to view agent. Rendering significantly slows down training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    agent.train()\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = torch.tensor(env.reset(),dtype=torch.float32)\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "            if episode>render_from:\n",
    "                env.render()\n",
    "            # Return agents action probability distribution\n",
    "            action_dist = agent.act(obs)\n",
    "            # Sample an action from this distribution\n",
    "            action = action_dist.sample()\n",
    "            # Take a step in the environment with the action drawn\n",
    "            next_obs , reward, done, info = env.step(action.numpy())\n",
    "            \n",
    "            next_obs = torch.tensor(next_obs,dtype=torch.float32)\n",
    "            \n",
    "            reward_total+=reward\n",
    "            \n",
    "            # Perform one step update\n",
    "            agent.update_model(obs,reward,action_dist.log_prob(action),next_obs,done)\n",
    "            \n",
    "            # Set the current state to the next state\n",
    "            obs = next_obs\n",
    "            \n",
    "            # if done then log and break\n",
    "            if done:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "        \n",
    "\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\n",
    "def eval(agent, env, num_episodes, num_steps, log_every=20):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to eval.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = env.reset()\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "           \n",
    "            env.render()\n",
    "            # Return agents action probability distribution\n",
    "            action_dist = agent.act(torch.tensor(obs,dtype=torch.float32))\n",
    "            # Sample an action from this distribution\n",
    "            action = action_dist.sample()\n",
    "            # Take a step in the environment with the action drawn\n",
    "            obs , reward, done, info = env.step(action.numpy())\n",
    "            # Just for logging\n",
    "            reward_total+=reward\n",
    "           \n",
    "            # if done then log and break\n",
    "            if done:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time to Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "if (isinstance(env.action_space,gym.spaces.Discrete)):\n",
    "    discrete = True\n",
    "    nb_actions = env.action_space.n\n",
    "else:\n",
    "    discrete = False\n",
    "    nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Hyper parameters\n",
    "HIDDEN_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0005\n",
    "EPISODES_TO_TRAIN = 700\n",
    "EPISODES_TO_EVAL = 20\n",
    "MAX_STEPS = 501\n",
    "LOG_EVERY = 20\n",
    "RENDER_FROM_EP = 2000\n",
    "\n",
    "agent = Actor_Critic_Agent(env.observation_space.shape[0],HIDDEN_SIZE,nb_actions,discrete,GAMMA,LEARNING_RATE)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "train(agent,env,EPISODES_TO_TRAIN,MAX_STEPS,LOG_EVERY,RENDER_FROM_EP)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "\n",
    "eval(agent,env,EPISODES_TO_EVAL,MAX_STEPS,log_every=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training...\n",
      "Episode   20 finished after   28 timesteps with a total reward of 28.0 | Running Average: 24.79\n",
      "Episode   40 finished after   21 timesteps with a total reward of 21.0 | Running Average: 27.54\n",
      "Episode   60 finished after   50 timesteps with a total reward of 50.0 | Running Average: 29.88\n",
      "Episode   80 finished after   30 timesteps with a total reward of 30.0 | Running Average: 35.58\n",
      "Episode  100 finished after   24 timesteps with a total reward of 24.0 | Running Average: 35.03\n",
      "Episode  120 finished after   32 timesteps with a total reward of 32.0 | Running Average: 38.75\n",
      "Episode  140 finished after   91 timesteps with a total reward of 91.0 | Running Average: 43.34\n",
      "Episode  160 finished after   46 timesteps with a total reward of 46.0 | Running Average: 52.12\n",
      "Episode  180 finished after   41 timesteps with a total reward of 41.0 | Running Average: 48.53\n",
      "Episode  200 finished after   28 timesteps with a total reward of 28.0 | Running Average: 52.21\n",
      "Episode  220 finished after   34 timesteps with a total reward of 34.0 | Running Average: 61.71\n",
      "Episode  240 finished after   43 timesteps with a total reward of 43.0 | Running Average: 58.21\n",
      "Episode  260 finished after   21 timesteps with a total reward of 21.0 | Running Average: 54.17\n",
      "Episode  280 finished after   69 timesteps with a total reward of 69.0 | Running Average: 59.06\n",
      "Episode  300 finished after   53 timesteps with a total reward of 53.0 | Running Average: 62.86\n",
      "Episode  320 finished after   73 timesteps with a total reward of 73.0 | Running Average: 62.00\n",
      "Episode  340 finished after   71 timesteps with a total reward of 71.0 | Running Average: 68.00\n",
      "Episode  360 finished after  147 timesteps with a total reward of 147.0 | Running Average: 76.08\n",
      "Episode  380 finished after  127 timesteps with a total reward of 127.0 | Running Average: 96.29\n",
      "Episode  400 finished after  248 timesteps with a total reward of 248.0 | Running Average: 124.73\n",
      "Episode  420 finished after  144 timesteps with a total reward of 144.0 | Running Average: 174.85\n",
      "Episode  440 finished after  364 timesteps with a total reward of 364.0 | Running Average: 196.58\n",
      "Episode  460 finished after  224 timesteps with a total reward of 224.0 | Running Average: 214.33\n",
      "Episode  480 finished after  127 timesteps with a total reward of 127.0 | Running Average: 221.45\n",
      "Episode  500 finished after  121 timesteps with a total reward of 121.0 | Running Average: 220.63\n",
      "Episode  520 finished after  148 timesteps with a total reward of 148.0 | Running Average: 178.44\n",
      "Episode  540 finished after  118 timesteps with a total reward of 118.0 | Running Average: 165.38\n",
      "Episode  560 finished after  136 timesteps with a total reward of 136.0 | Running Average: 152.08\n",
      "Episode  580 finished after  120 timesteps with a total reward of 120.0 | Running Average: 154.45\n",
      "Episode  600 finished after  197 timesteps with a total reward of 197.0 | Running Average: 167.07\n",
      "Episode  620 finished after  148 timesteps with a total reward of 148.0 | Running Average: 190.11\n",
      "Episode  640 finished after  105 timesteps with a total reward of 105.0 | Running Average: 197.35\n",
      "Episode  660 finished after  211 timesteps with a total reward of 211.0 | Running Average: 203.56\n",
      "Episode  680 finished after  144 timesteps with a total reward of 144.0 | Running Average: 190.54\n",
      "Episode  700 finished after  148 timesteps with a total reward of 148.0 | Running Average: 172.99\n",
      "Evaluating...\n",
      "Episode    2 finished after  154 timesteps with a total reward of 154.0 | Running Average: 153.00\n",
      "Episode    3 finished after  165 timesteps with a total reward of 165.0 | Running Average: 153.50\n",
      "Episode    4 finished after  205 timesteps with a total reward of 205.0 | Running Average: 157.33\n",
      "Episode    5 finished after  173 timesteps with a total reward of 173.0 | Running Average: 169.25\n",
      "Episode    6 finished after  153 timesteps with a total reward of 153.0 | Running Average: 170.00\n",
      "Episode    7 finished after  149 timesteps with a total reward of 149.0 | Running Average: 167.17\n",
      "Episode    8 finished after  194 timesteps with a total reward of 194.0 | Running Average: 164.57\n",
      "Episode    9 finished after  145 timesteps with a total reward of 145.0 | Running Average: 168.25\n",
      "Episode   10 finished after  152 timesteps with a total reward of 152.0 | Running Average: 165.67\n",
      "Episode   11 finished after  162 timesteps with a total reward of 162.0 | Running Average: 164.30\n",
      "Episode   12 finished after  148 timesteps with a total reward of 148.0 | Running Average: 164.09\n",
      "Episode   13 finished after  136 timesteps with a total reward of 136.0 | Running Average: 162.75\n",
      "Episode   14 finished after  144 timesteps with a total reward of 144.0 | Running Average: 160.69\n",
      "Episode   15 finished after  130 timesteps with a total reward of 130.0 | Running Average: 159.50\n",
      "Episode   16 finished after  190 timesteps with a total reward of 190.0 | Running Average: 157.53\n",
      "Episode   17 finished after  134 timesteps with a total reward of 134.0 | Running Average: 159.56\n",
      "Episode   18 finished after  145 timesteps with a total reward of 145.0 | Running Average: 158.06\n",
      "Episode   19 finished after  159 timesteps with a total reward of 159.0 | Running Average: 157.33\n",
      "Episode   20 finished after  140 timesteps with a total reward of 140.0 | Running Average: 157.42\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "05a25e9113b15a9f940c6517e4f0bb1aaafcc84783dc3502a8f30808da99fb6f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}