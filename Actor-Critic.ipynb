{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Actor-Critic\n",
    "\n",
    "The Actor-Critic framework can be seen as a merging of policy-based methods and value-based methods in the hopes of achieving the benefits of both. \n",
    "\n",
    "It is beneficial to learn the value function as well as the policy since knowing the value function can assist and push the policy updates in the correct direction and with less variance than Monte Carlo estimates. This also allows policy methods to be used in non-episodic environments as well as be updated more frequently as it doesn't need to wait until the end of the episode.\n",
    "\n",
    "Below is the Pseudocode for a one-step actor-critic:\n",
    "\n",
    "![Pseudocode](https://i.stack.imgur.com/zFfxs.png)\n",
    "\n",
    "The following will be an implementation of a one-step actor-critic but there are many other ways to do this. Instead of bootstrapping one could use Monte Carlo estimates to update the critic and update the policy on a whole episode of experience.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discrete Policy\n",
    "The discete action space policy is simply a neural network with the following layer sizes:\n",
    "- Input : State Observation size\n",
    "- Hidden : Whatever you please.\n",
    "- Output: Number of possible actions.\n",
    "\n",
    "The policy then uses a softmax activation function at the end to give probabilities for selecting an action."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Discrete_Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Discrete_Policy, self).__init__()\n",
    "        self.policy_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions),\n",
    "                                        nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.policy_net(obs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Continuous Policy\n",
    "The continuous action space policy is a little more complicated. This can be done differently depending on what you want to do but the following uses two separate neural networks to approximate a mean and std. deviation respectively.\n",
    "\n",
    "The sizes of both neural networks are as follows:\n",
    "\n",
    "- Input : State Observation size\n",
    "- Hidden : Whatever you please.\n",
    "- Output: 1\n",
    "\n",
    "One doesnt need to use a separate neural network for the std deviation, they can use a single learned parameter or even some fixed constant but all will have different results and this involves some experimentation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "class Continuous_Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Continuous_Policy, self).__init__()\n",
    "        self.mean_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions))\n",
    "        self.standard_deviation_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.mean_net(obs),torch.abs(self.standard_deviation_net(obs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Critic\n",
    "This is the critic model - a neural network to learn the value function. This could be a Q (action-value) or V (state-value) approximator. This depends on implementation - here I am creating a state-value approximator."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        self.critic = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, 1))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.critic(obs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent \n",
    "The following is the agent class:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "class Actor_Critic_Agent:\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,nb_actions,discrete=True,discount_factor=0.9,learning_rate=0.0001):\n",
    "        self.discrete = discrete\n",
    "        if discrete:\n",
    "            self.policy = Discrete_Policy(input_size,hidden_size,nb_actions)\n",
    "        else:\n",
    "            self.policy = Continuous_Policy(input_size,hidden_size,nb_actions)\n",
    "        \n",
    "\n",
    "        self.critic = Critic(input_size,hidden_size)\n",
    "\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        # One can use one optimizer for both the critic and policy - this is a implementation detail\n",
    "        self.policy_optimizer = AdamW(self.policy.parameters(),learning_rate)\n",
    "        self.critic_optimizer = AdamW(self.critic.parameters(),learning_rate)\n",
    "\n",
    "        self.best_weights = self.policy.state_dict()\n",
    "        \n",
    "    def act(self,obs):\n",
    "        if self.discrete:\n",
    "            # Create categorical distribution using the action probabilities given by the agent's policy\n",
    "            action_dist = Categorical(self.policy(obs))\n",
    "\n",
    "        else:\n",
    "            mu,dev = self.policy(obs)\n",
    "            # Create a Normal distribution using the policy given mean and std deviation\n",
    "            action_dist = Normal(mu,dev)\n",
    "        \n",
    "        # Return the action distribution created according to policy\n",
    "        return action_dist\n",
    "    \n",
    "    def train(self):\n",
    "        self.policy.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.policy.load_state_dict(self.best_weights)\n",
    "        self.policy.eval()\n",
    "        \n",
    "    def update_model(self,state,reward,log_action_probability,next_state,done):\n",
    "        \n",
    "        # Current State Value\n",
    "        state_value = self.critic(state)\n",
    "\n",
    "        # Target State Value\n",
    "        target_value = reward+ (1-done)*self.gamma*self.critic(next_state)\n",
    "        \n",
    "        # Temporal Difference Error\n",
    "        td_error = target_value - state_value\n",
    "\n",
    "        # Critics Loss - Huber loss of state and target - can use other loss functions such as MSE\n",
    "        critic_loss = F.smooth_l1_loss(state_value,target_value)\n",
    "\n",
    "        # Policy Loss - log action probability shifted by the td_error\n",
    "        policy_loss = -td_error.detach()*log_action_probability\n",
    "\n",
    "        # clear the optimizers current gradients\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "\n",
    "        # backpropagate the loss to calculate the gradients\n",
    "        critic_loss.backward()\n",
    "        policy_loss.backward()\n",
    "\n",
    "        # make one optimization step\n",
    "        self.policy_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "    \n",
    "    def update_best_weights(self):\n",
    "        self.best_weights = self.policy.state_dict()\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "The last thing needed is the training loop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "from numpy import double\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "def train(agent, env, num_episodes, num_steps, log_every=20, render_from=1000):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to train.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    :param render_from: The episode number to start rendering to screen to allow one to view agent. Rendering significantly slows down training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    best_running_avg = 0\n",
    "    agent.train()\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = torch.tensor(env.reset(),dtype=torch.float32)\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "            if episode>render_from:\n",
    "                env.render()\n",
    "            # Return agents action probability distribution\n",
    "            action_dist = agent.act(obs)\n",
    "            # Sample an action from this distribution\n",
    "            action = action_dist.sample()\n",
    "            # Take a step in the environment with the action drawn\n",
    "            next_obs , reward, done, info = env.step(action.numpy())\n",
    "            \n",
    "            next_obs = torch.tensor(next_obs,dtype=torch.float32)\n",
    "            \n",
    "            reward_total+=reward\n",
    "            \n",
    "            # Perform one step update\n",
    "            agent.update_model(obs,reward,action_dist.log_prob(action),next_obs,done)\n",
    "            \n",
    "            # Set the current state to the next state\n",
    "            obs = next_obs\n",
    "            \n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    if running_avg > best_running_avg:\n",
    "                        best_running_avg = running_avg\n",
    "                        agent.update_best_weights()\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "        \n",
    "\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "\n",
    "def eval(agent, env, num_episodes, num_steps, log_every=20):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to eval.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = env.reset()\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "           \n",
    "            env.render()\n",
    "            # Return agents action probability distribution\n",
    "            action_dist = agent.act(torch.tensor(obs,dtype=torch.float32))\n",
    "            # Sample an action from this distribution\n",
    "            action = action_dist.sample()\n",
    "            # Take a step in the environment with the action drawn\n",
    "            obs , reward, done, info = env.step(action.numpy())\n",
    "            # Just for logging\n",
    "            reward_total+=reward\n",
    "           \n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time to Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "if (isinstance(env.action_space,gym.spaces.Discrete)):\n",
    "    discrete = True\n",
    "    nb_actions = env.action_space.n\n",
    "else:\n",
    "    discrete = False\n",
    "    nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Hyper parameters\n",
    "HIDDEN_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0001\n",
    "EPISODES_TO_TRAIN = 1000\n",
    "EPISODES_TO_EVAL = 20\n",
    "MAX_STEPS = 201\n",
    "LOG_EVERY = 20\n",
    "RENDER_FROM_EP = 2000\n",
    "\n",
    "agent = Actor_Critic_Agent(env.observation_space.shape[0],HIDDEN_SIZE,nb_actions,discrete,GAMMA,LEARNING_RATE)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "train(agent,env,EPISODES_TO_TRAIN,MAX_STEPS,LOG_EVERY,RENDER_FROM_EP)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "\n",
    "eval(agent,env,EPISODES_TO_EVAL,MAX_STEPS,log_every=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training...\n",
      "Episode   20 finished after   27 timesteps with a total reward of 27.0 | Running Average: 21.16\n",
      "Episode   40 finished after   17 timesteps with a total reward of 17.0 | Running Average: 20.95\n",
      "Episode   60 finished after   25 timesteps with a total reward of 25.0 | Running Average: 20.32\n",
      "Episode   80 finished after   18 timesteps with a total reward of 18.0 | Running Average: 20.66\n",
      "Episode  100 finished after   23 timesteps with a total reward of 23.0 | Running Average: 20.89\n",
      "Episode  120 finished after   16 timesteps with a total reward of 16.0 | Running Average: 22.16\n",
      "Episode  140 finished after   71 timesteps with a total reward of 71.0 | Running Average: 24.23\n",
      "Episode  160 finished after   33 timesteps with a total reward of 33.0 | Running Average: 27.40\n",
      "Episode  180 finished after   21 timesteps with a total reward of 21.0 | Running Average: 31.43\n",
      "Episode  200 finished after   41 timesteps with a total reward of 41.0 | Running Average: 36.78\n",
      "Episode  220 finished after   56 timesteps with a total reward of 56.0 | Running Average: 42.46\n",
      "Episode  240 finished after   51 timesteps with a total reward of 51.0 | Running Average: 48.58\n",
      "Episode  260 finished after   79 timesteps with a total reward of 79.0 | Running Average: 52.68\n",
      "Episode  280 finished after   31 timesteps with a total reward of 31.0 | Running Average: 56.33\n",
      "Episode  300 finished after   95 timesteps with a total reward of 95.0 | Running Average: 56.76\n",
      "Episode  320 finished after  171 timesteps with a total reward of 171.0 | Running Average: 61.16\n",
      "Episode  340 finished after   67 timesteps with a total reward of 67.0 | Running Average: 70.29\n",
      "Episode  360 finished after  128 timesteps with a total reward of 128.0 | Running Average: 73.32\n",
      "Episode  380 finished after   40 timesteps with a total reward of 40.0 | Running Average: 74.04\n",
      "Episode  400 finished after   59 timesteps with a total reward of 59.0 | Running Average: 73.63\n",
      "Episode  420 finished after   34 timesteps with a total reward of 34.0 | Running Average: 66.70\n",
      "Episode  440 finished after   51 timesteps with a total reward of 51.0 | Running Average: 56.01\n",
      "Episode  460 finished after   35 timesteps with a total reward of 35.0 | Running Average: 56.77\n",
      "Episode  480 finished after  172 timesteps with a total reward of 172.0 | Running Average: 72.23\n",
      "Episode  500 finished after   76 timesteps with a total reward of 76.0 | Running Average: 89.94\n",
      "Episode  520 finished after  107 timesteps with a total reward of 107.0 | Running Average: 104.46\n",
      "Episode  540 finished after   76 timesteps with a total reward of 76.0 | Running Average: 108.44\n",
      "Episode  560 finished after  200 timesteps with a total reward of 200.0 | Running Average: 122.50\n",
      "Episode  580 finished after  200 timesteps with a total reward of 200.0 | Running Average: 130.80\n",
      "Episode  600 finished after  130 timesteps with a total reward of 130.0 | Running Average: 140.61\n",
      "Episode  620 finished after  149 timesteps with a total reward of 149.0 | Running Average: 151.72\n",
      "Episode  640 finished after  158 timesteps with a total reward of 158.0 | Running Average: 170.38\n",
      "Episode  660 finished after  200 timesteps with a total reward of 200.0 | Running Average: 176.56\n",
      "Episode  680 finished after   84 timesteps with a total reward of 84.0 | Running Average: 166.83\n",
      "Episode  700 finished after  100 timesteps with a total reward of 100.0 | Running Average: 151.67\n",
      "Episode  720 finished after  109 timesteps with a total reward of 109.0 | Running Average: 141.71\n",
      "Episode  740 finished after  189 timesteps with a total reward of 189.0 | Running Average: 134.12\n",
      "Episode  760 finished after  200 timesteps with a total reward of 200.0 | Running Average: 138.20\n",
      "Episode  780 finished after  182 timesteps with a total reward of 182.0 | Running Average: 149.85\n",
      "Episode  800 finished after  200 timesteps with a total reward of 200.0 | Running Average: 165.51\n",
      "Episode  820 finished after   47 timesteps with a total reward of 47.0 | Running Average: 170.41\n",
      "Episode  840 finished after  129 timesteps with a total reward of 129.0 | Running Average: 167.06\n",
      "Episode  860 finished after   72 timesteps with a total reward of 72.0 | Running Average: 145.46\n",
      "Episode  880 finished after  200 timesteps with a total reward of 200.0 | Running Average: 136.01\n",
      "Episode  900 finished after  145 timesteps with a total reward of 145.0 | Running Average: 125.82\n",
      "Episode  920 finished after   99 timesteps with a total reward of 99.0 | Running Average: 121.68\n",
      "Episode  940 finished after   68 timesteps with a total reward of 68.0 | Running Average: 128.15\n",
      "Episode  960 finished after  200 timesteps with a total reward of 200.0 | Running Average: 138.93\n",
      "Episode  980 finished after  200 timesteps with a total reward of 200.0 | Running Average: 148.58\n",
      "Episode 1000 finished after   90 timesteps with a total reward of 90.0 | Running Average: 159.02\n",
      "Evaluating...\n",
      "Episode    2 finished after   55 timesteps with a total reward of 55.0 | Running Average: 28.00\n",
      "Episode    3 finished after  178 timesteps with a total reward of 178.0 | Running Average: 41.50\n",
      "Episode    4 finished after  200 timesteps with a total reward of 200.0 | Running Average: 87.00\n",
      "Episode    5 finished after  200 timesteps with a total reward of 200.0 | Running Average: 115.25\n",
      "Episode    6 finished after  200 timesteps with a total reward of 200.0 | Running Average: 132.20\n",
      "Episode    7 finished after  151 timesteps with a total reward of 151.0 | Running Average: 143.50\n",
      "Episode    8 finished after  163 timesteps with a total reward of 163.0 | Running Average: 144.57\n",
      "Episode    9 finished after   78 timesteps with a total reward of 78.0 | Running Average: 146.88\n",
      "Episode   10 finished after   92 timesteps with a total reward of 92.0 | Running Average: 139.22\n",
      "Episode   11 finished after  171 timesteps with a total reward of 171.0 | Running Average: 134.50\n",
      "Episode   12 finished after  194 timesteps with a total reward of 194.0 | Running Average: 137.82\n",
      "Episode   13 finished after  200 timesteps with a total reward of 200.0 | Running Average: 142.50\n",
      "Episode   14 finished after  200 timesteps with a total reward of 200.0 | Running Average: 146.92\n",
      "Episode   15 finished after  200 timesteps with a total reward of 200.0 | Running Average: 150.71\n",
      "Episode   16 finished after  200 timesteps with a total reward of 200.0 | Running Average: 154.00\n",
      "Episode   17 finished after  200 timesteps with a total reward of 200.0 | Running Average: 156.88\n",
      "Episode   18 finished after   66 timesteps with a total reward of 66.0 | Running Average: 159.41\n",
      "Episode   19 finished after  122 timesteps with a total reward of 122.0 | Running Average: 154.22\n",
      "Episode   20 finished after   88 timesteps with a total reward of 88.0 | Running Average: 152.53\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "05a25e9113b15a9f940c6517e4f0bb1aaafcc84783dc3502a8f30808da99fb6f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}