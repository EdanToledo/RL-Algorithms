{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Actor-Critic\n",
    "\n",
    "The Actor-Critic framework can be seen as a merging of policy-based methods and value-based methods in the hopes of achieving the benefits of both. \n",
    "\n",
    "It is beneficial to learn the value function as well as the policy since knowing the value function can assist and push the policy updates in the correct direction and with less variance than Monte Carlo estimates. This also allows policy methods to be used in non-episodic environments as well as be updated more frequently as it doesn't need to wait until the end of the episode.\n",
    "\n",
    "Below is the Pseudocode for a one-step actor-critic:\n",
    "\n",
    "![Pseudocode](https://i.stack.imgur.com/zFfxs.png)\n",
    "\n",
    "The following will be an implementation of a one-step actor-critic but there are many other ways to do this. Instead of bootstrapping one could use Monte Carlo estimates to update the critic and update the policy on a whole episode of experience.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discrete Policy\n",
    "The discete action space policy is simply a neural network with the following layer sizes:\n",
    "- Input : State Observation size\n",
    "- Hidden : Whatever you please.\n",
    "- Output: Number of possible actions.\n",
    "\n",
    "The policy then uses a softmax activation function at the end to give probabilities for selecting an action."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Discrete_Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Discrete_Policy, self).__init__()\n",
    "        self.policy_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions),\n",
    "                                        nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.policy_net(obs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Continuous Policy\n",
    "The continuous action space policy is a little more complicated. This can be done differently depending on what you want to do but the following uses two separate neural networks to approximate a mean and std. deviation respectively.\n",
    "\n",
    "The sizes of both neural networks are as follows:\n",
    "\n",
    "- Input : State Observation size\n",
    "- Hidden : Whatever you please.\n",
    "- Output: 1\n",
    "\n",
    "One doesnt need to use a separate neural network for the std deviation, they can use a single learned parameter or even some fixed constant but all will have different results and this involves some experimentation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Continuous_Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Continuous_Policy, self).__init__()\n",
    "        self.mean_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions))\n",
    "        self.standard_deviation_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.mean_net(obs),torch.abs(self.standard_deviation_net(obs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Critic\n",
    "This is the critic model - a neural network to learn the value function. This could be a Q (action-value) or V (state-value) approximator. This depends on implementation - here I am creating a state-value approximator."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        self.critic = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, 1))\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.critic(obs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent \n",
    "The following is the agent class:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "class Actor_Critic_Agent:\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,nb_actions,discrete=True,discount_factor=0.9,learning_rate=0.0001):\n",
    "        self.discrete = discrete\n",
    "        if discrete:\n",
    "            self.policy = Discrete_Policy(input_size,hidden_size,nb_actions)\n",
    "        else:\n",
    "            self.policy = Continuous_Policy(input_size,hidden_size,nb_actions)\n",
    "        \n",
    "\n",
    "        self.critic = Critic(input_size,hidden_size)\n",
    "\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        # One can use one optimizer for both the critic and policy - this is a implementation detail\n",
    "        self.policy_optimizer = AdamW(self.policy.parameters(),learning_rate)\n",
    "        self.critic_optimizer = AdamW(self.critic.parameters(),learning_rate)\n",
    "\n",
    "        self.best_weights = self.policy.state_dict()\n",
    "        \n",
    "    def act(self,obs):\n",
    "        if self.discrete:\n",
    "            # Create categorical distribution using the action probabilities given by the agent's policy\n",
    "            action_dist = Categorical(self.policy(obs))\n",
    "\n",
    "        else:\n",
    "            mu,dev = self.policy(obs)\n",
    "            # Create a Normal distribution using the policy given mean and std deviation\n",
    "            action_dist = Normal(mu,dev)\n",
    "        \n",
    "        # Return the action distribution created according to policy\n",
    "        return action_dist\n",
    "    \n",
    "    def train(self):\n",
    "        self.policy.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.policy.load_state_dict(self.best_weights)\n",
    "        self.policy.eval()\n",
    "        \n",
    "    def update_model(self,state,reward,log_action_probability,next_state,done):\n",
    "        \n",
    "        # Current State Value\n",
    "        state_value = self.critic(state)\n",
    "\n",
    "        # Target State Value\n",
    "        target_value = reward+ (1-done)*self.gamma*self.critic(next_state)\n",
    "        \n",
    "        # Temporal Difference Error\n",
    "        td_error = target_value - state_value\n",
    "\n",
    "        # Critics Loss - Huber loss of state and target - can use other loss functions such as MSE\n",
    "        critic_loss = F.smooth_l1_loss(state_value,target_value)\n",
    "\n",
    "        # Policy Loss - log action probability shifted by the td_error\n",
    "        policy_loss = -td_error.detach()*log_action_probability\n",
    "\n",
    "        # clear the optimizers current gradients\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "\n",
    "        # backpropagate the loss to calculate the gradients\n",
    "        critic_loss.backward()\n",
    "        policy_loss.backward()\n",
    "\n",
    "        # make one optimization step\n",
    "        self.policy_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "    \n",
    "    def update_best_weights(self):\n",
    "        self.best_weights = self.policy.state_dict()\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "The last thing needed is the training loop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "from numpy import double\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "def train(agent, env, num_episodes, num_steps, log_every=20, render_from=1000):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to train.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    :param render_from: The episode number to start rendering to screen to allow one to view agent. Rendering significantly slows down training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    best_running_avg = 0\n",
    "    agent.train()\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = torch.tensor(env.reset(),dtype=torch.float32)\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "            if episode>render_from:\n",
    "                env.render()\n",
    "            # Return agents action probability distribution\n",
    "            action_dist = agent.act(obs)\n",
    "            # Sample an action from this distribution\n",
    "            action = action_dist.sample()\n",
    "            # Take a step in the environment with the action drawn\n",
    "            next_obs , reward, done, info = env.step(action.numpy())\n",
    "            \n",
    "            next_obs = torch.tensor(next_obs,dtype=torch.float32)\n",
    "            \n",
    "            reward_total+=reward\n",
    "            \n",
    "            # Perform one step update\n",
    "            agent.update_model(obs,reward,action_dist.log_prob(action),next_obs,done)\n",
    "            \n",
    "            # Set the current state to the next state\n",
    "            obs = next_obs\n",
    "            \n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    if running_avg > best_running_avg:\n",
    "                        best_running_avg = running_avg\n",
    "                        agent.update_best_weights()\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "        \n",
    "\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def eval(agent, env, num_episodes, num_steps, log_every=20):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to eval.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = env.reset()\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "           \n",
    "            env.render()\n",
    "            # Return agents action probability distribution\n",
    "            action_dist = agent.act(torch.tensor(obs,dtype=torch.float32))\n",
    "            # Sample an action from this distribution\n",
    "            action = action_dist.sample()\n",
    "            # Take a step in the environment with the action drawn\n",
    "            obs , reward, done, info = env.step(action.numpy())\n",
    "            # Just for logging\n",
    "            reward_total+=reward\n",
    "           \n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time to Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "if (isinstance(env.action_space,gym.spaces.Discrete)):\n",
    "    discrete = True\n",
    "    nb_actions = env.action_space.n\n",
    "else:\n",
    "    discrete = False\n",
    "    nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Hyper parameters\n",
    "HIDDEN_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0001\n",
    "EPISODES_TO_TRAIN = 1000\n",
    "EPISODES_TO_EVAL = 20\n",
    "MAX_STEPS = 201\n",
    "LOG_EVERY = 20\n",
    "RENDER_FROM_EP = 2000\n",
    "\n",
    "agent = Actor_Critic_Agent(env.observation_space.shape[0],HIDDEN_SIZE,nb_actions,discrete,GAMMA,LEARNING_RATE)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "train(agent,env,EPISODES_TO_TRAIN,MAX_STEPS,LOG_EVERY,RENDER_FROM_EP)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "\n",
    "eval(agent,env,EPISODES_TO_EVAL,MAX_STEPS,log_every=1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "05a25e9113b15a9f940c6517e4f0bb1aaafcc84783dc3502a8f30808da99fb6f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}