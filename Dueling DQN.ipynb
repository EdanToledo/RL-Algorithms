{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dueling DQN\n",
    "\n",
    "Dueling Deep Q-Networks takes a new approach to estimating Q values. One can decompose the Q value formula into two parts:\n",
    "1. The advantage function - The advantage of the taking an action in that state compared to other actions : $A(s,a) = Q(s,a) - V(s)$\n",
    "2. The state value function - The value of the state i.e the expected cumulative reward to be recieved following a given policy : $V(s) = \\sum_{a \\in \\mathcal{A}}Q(s,a)$\n",
    "\n",
    "From the advantage function we can see that $Q(s,a) = A(s,a)+V(s)$ so what dueling DQN does, is separate the estimator into two streams - one being the Advantage function and the other being the state-value function and then combine these through an special aggregation layer to get the estimate of the Q function. \n",
    "\n",
    "![Architecture](https://paperswithcode.com/media/methods/Screen_Shot_2020-06-03_at_3.24.01_PM.png)\n",
    "\n",
    "The reason for this decoupling of estimators is so that the DQN can learn separately which states are or are not valuable without the need to learn the effect of each action at each state. The authors stated that there is no point in learning/calculating action values if the entire state value is bad and can be avoided e.g why calculate all actions at one state when all these actions lead to death?\n",
    "\n",
    "This decoupling is also useful in states where actions have no impact on the environment in any important or relevant way. In this situation there is no need to calculate the value for each action.\n",
    "\n",
    "![Example](https://miro.medium.com/max/928/1*TJrgh57v6919Ck5ZGSWZ4g.png)\n",
    "\n",
    "The figure above is an example of the benefit - the advantage stream only learns to pay attention when cars are immediately in front of it so that it can avoid collisions but otherwise it is unnecessary to learn as actions have no relevant consequence on the environment.\n",
    "\n",
    "The aggregation layer is important as it is the Q value is ultimately calculated - one might think it is as simple as $Q(s,a) = A(s,a)+V(s)$ but the issue with this is one of identifiability. Given a Q value we can't find A and V which is a problem for backpropagation. \n",
    "\n",
    "The authors propose forcing the advantage function to have zero advantage at the chosen action by using the following formula:\n",
    "$Q(s,a) = V(s)+(A(s,a)-\\max_{a` \\in |\\mathcal{A}|}A(s,a`))$\n",
    "\n",
    "An alternative method that replaces the max operator with an average:\n",
    "$Q(s,a) = V(s)+(A(s,a)-\\frac{1}{|\\mathcal{A}|}\\sum_{a`}A(s,a`))$\n",
    "\n",
    "This replacement loses the original semantics of V and A since it puts them off target by a constant - but this increases the stability of the optimization since advantages only need to change as fast as the mean instead of having to compensate any change to the optimal actions advantage. This is ultimately what the authors used in their paper so this implementation will use this too."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dueling DQN Network\n",
    "\n",
    "The Dueling DQN takes in the state input and outputs Q values for all possible actions using the two streams and aggregator layer. There is a common feature layer and then two separate heads - This architecture might be a little deep which can slow learning down for simple problems - try removing a hidden layer from the stream heads and see if it can increase learning speed. e.g remove the first linear layer in each of the heads."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Dueling_DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Dueling_DQN, self).__init__()\n",
    "        self.feature_layer = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU())\n",
    "\n",
    "        self.value_function = nn.Sequential(\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, 1),\n",
    "                                        nn.LeakyReLU())\n",
    "\n",
    "        self.advantage_function = nn.Sequential(\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions),\n",
    "                                        nn.LeakyReLU())\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        feature_state = self.feature_layer(obs)\n",
    "        state_value = self.value_function(feature_state)\n",
    "        advantage_values = self.advantage_function(feature_state)\n",
    "        q_values = state_value + (advantage_values - advantage_values.mean())\n",
    "        \n",
    "        return q_values\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replay Memory\n",
    "\n",
    "DQNs make use of experience replay memory - this stores previous transitions to allow for data efficient and non correlated learning to take place."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", (\"state\", \"action\", \"reward\", \"next_state\", \"done\"))\n",
    "\n",
    "class Replay_Memory:\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque([], maxlen=size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.memory):\n",
    "            return None\n",
    "        return random.sample(self.memory, batch_size)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent \n",
    "The following is the agent class:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "class Dueling_DQN_Agent:\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,nb_actions,discount_factor=0.9,learning_rate=0.0001,epsilon_start=0.9,epsilon_end=0.05,epsilon_anneal_over_steps=10000,batch_size=256,update_frequency = 1000,replay_memory_size=100000):\n",
    "        \n",
    "        # Create the online network that the agent uses to select actions\n",
    "        self.online_net = Dueling_DQN(input_size,hidden_size,nb_actions)\n",
    "\n",
    "        # Create the target network that the agent uses in it's updates\n",
    "        self.target_net = Dueling_DQN(input_size,hidden_size,nb_actions)\n",
    "\n",
    "        self.num_actions = nb_actions\n",
    "\n",
    "        # Set the target net's initial weights to equal the online net\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        # User Huber Loss function - others can be use such as MSE\n",
    "        self.loss_function = torch.nn.SmoothL1Loss()\n",
    "\n",
    "        # DQN optimizer\n",
    "        self.optimizer = AdamW(self.online_net.parameters(),learning_rate)\n",
    "        \n",
    "        # Set initial and final epsilon values for epsilon greedy and choose the number of environment steps to anneal over\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_anneal_over_steps = epsilon_anneal_over_steps\n",
    "        self.step_no = 0\n",
    "\n",
    "        # Create Replay Memory and assign batch_size\n",
    "        self.replay = Replay_Memory(replay_memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Set update Frequency\n",
    "        self.update_frequency = update_frequency\n",
    "        \n",
    "        # Best Weights for Eval\n",
    "        self.best_weights = self.online_net.state_dict()\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        \"\"\"\n",
    "        Get current epsilon value according to agents total step number in the environment\n",
    "        \"\"\"\n",
    "        eps = self.epsilon_end\n",
    "        if self.step_no < self.epsilon_anneal_over_steps:\n",
    "            eps = self.epsilon_start - self.step_no * \\\n",
    "                ((self.epsilon_start - self.epsilon_end) /\n",
    "                 self.epsilon_anneal_over_steps)\n",
    "        return eps\n",
    "        \n",
    "    def act(self,obs):\n",
    "        # Increment global step count\n",
    "        self.step_no+=1\n",
    "\n",
    "        if np.random.uniform() > self.get_epsilon():\n",
    "            # Dont store gradients when acting in the environment\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.online_net(obs),dim=-1).view(1)\n",
    "        else:\n",
    "            return torch.tensor([random.randrange(self.num_actions)], dtype=torch.long)\n",
    "\n",
    "    # store experience in replay memory\n",
    "    def cache(self, state, action, reward, next_state, done):\n",
    "        self.replay.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def train(self):\n",
    "        self.online_net.train()\n",
    "        self.target_net.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.online_net.load_state_dict(self.best_weights)\n",
    "        self.online_net.eval()\n",
    "        self.target_net.eval()\n",
    "\n",
    "    def update_model(self):\n",
    "\n",
    "        # Get minibatch of data from experience buffer\n",
    "        batch = self.replay.sample(self.batch_size)\n",
    "\n",
    "        # If memory doesnt have enough transitions\n",
    "        if batch == None:\n",
    "            return\n",
    "\n",
    "        # Format batch to get a tensor of states, actions, rewards, next states and done booleans\n",
    "        batch_tuple = Transition(*zip(*batch))\n",
    "        state = torch.stack(batch_tuple.state)\n",
    "        action = torch.stack(batch_tuple.action)\n",
    "        reward = torch.stack(batch_tuple.reward)\n",
    "        next_state = torch.stack(batch_tuple.next_state)\n",
    "        done = torch.stack(batch_tuple.done)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Get the Q values of the online nets current state and actions\n",
    "        Q_Values = self.online_net(state).gather(1, action).squeeze()\n",
    "\n",
    "        # Get the max Q values of the target nets next state\n",
    "        Q_Targets = reward + (1 - done.float()) * self.gamma * self.target_net(next_state).max(dim=-1)[0].detach()\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.loss_function(Q_Values, Q_Targets)\n",
    "\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization step\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "\n",
    "    def update_target(self):\n",
    "        \"\"\"\n",
    "        Update the target nets weights\n",
    "        \"\"\"\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "    def update_best_weights(self):\n",
    "        self.best_weights = self.online_net.state_dict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "The last thing needed is the training loop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "from numpy import double\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "def train(agent, env, num_episodes, num_steps, log_every=20, render_from=1000):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to train.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    :param render_from: The episode number to start rendering to screen to allow one to view agent. Rendering significantly slows down training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    best_running_avg = 0\n",
    "    agent.train()\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = torch.tensor(env.reset(),dtype=torch.float32)\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "            if episode>render_from:\n",
    "                env.render()\n",
    "\n",
    "            # return chosen action\n",
    "            action = agent.act(obs)\n",
    "            \n",
    "            # Take a step in the environment with the action drawn\n",
    "            next_obs , reward, done, info = env.step(action.item())\n",
    "            \n",
    "            # Just for logging\n",
    "            reward_total+=reward\n",
    "\n",
    "            # change next state into tensor for update\n",
    "            next_obs = torch.tensor(next_obs,dtype=torch.float32)\n",
    "            \n",
    "            # change reward into tensor for update\n",
    "            reward = torch.tensor(\n",
    "                reward, dtype=torch.float32)\n",
    "\n",
    "            # Store transition in replay memory\n",
    "            agent.cache(obs,action,reward,next_obs,torch.tensor(done))\n",
    "\n",
    "            # Perform update\n",
    "            agent.update_model()\n",
    "            \n",
    "            # If the number of steps has elapsed then perform update\n",
    "            if agent.step_no % agent.update_frequency == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "            # Set the current state to the next state\n",
    "            obs = next_obs\n",
    "            \n",
    "            # if done then log and break\n",
    "            if done:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    if running_avg > best_running_avg:\n",
    "                        best_running_avg = running_avg\n",
    "                        agent.update_best_weights()\n",
    "                        \n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "        \n",
    "\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def eval(agent, env, num_episodes, num_steps, log_every=20):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to eval.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = torch.tensor(env.reset(),dtype=torch.float32)\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "           \n",
    "            env.render()\n",
    "            # return chosen action\n",
    "            action = agent.act(obs)\n",
    "            \n",
    "            # Take a step in the environment with the action drawn\n",
    "            next_obs , reward, done, info = env.step(action.item())\n",
    "            \n",
    "            # Just for logging\n",
    "            reward_total+=reward\n",
    "\n",
    "            # change next state into tensor for update\n",
    "            next_obs = torch.tensor(next_obs,dtype=torch.float32)\n",
    "            \n",
    "            # Set the current state to the next state\n",
    "            obs = next_obs\n",
    "           \n",
    "            # if done then log and break\n",
    "            if done:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time to Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Hyper parameters\n",
    "HIDDEN_SIZE = 512\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0005\n",
    "EPISODES_TO_TRAIN = 700\n",
    "EPISODES_TO_EVAL = 20\n",
    "MAX_STEPS = 201\n",
    "LOG_EVERY = 20\n",
    "RENDER_FROM_EP = 2000\n",
    "EPSILON_START=0.9\n",
    "EPSILON_END=0.01\n",
    "EPSILON_ANNEAL_OVER_STEPS=10000\n",
    "BATCH_SIZE=512\n",
    "UPDATE_FREQUENCY = 1500\n",
    "REPLAY_MEMORY_SIZE=500000\n",
    "\n",
    "\n",
    "agent = Dueling_DQN_Agent(env.observation_space.shape[0],HIDDEN_SIZE,nb_actions,GAMMA,LEARNING_RATE,EPSILON_START,EPSILON_END,EPSILON_ANNEAL_OVER_STEPS,BATCH_SIZE,UPDATE_FREQUENCY,REPLAY_MEMORY_SIZE)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "train(agent,env,EPISODES_TO_TRAIN,MAX_STEPS,LOG_EVERY,RENDER_FROM_EP)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "\n",
    "eval(agent,env,EPISODES_TO_EVAL,MAX_STEPS,log_every=1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "05a25e9113b15a9f940c6517e4f0bb1aaafcc84783dc3502a8f30808da99fb6f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}