{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DQN\n",
    "\n",
    "DQN (Deep Q-Network) is a method to implement Q learning - an off policy value based method - using deep neural networks. Q learning when using non linear function approximators can be very unstable and data inefficient so DQN addresses these problems. [DQN Paper](https://arxiv.org/pdf/1312.5602.pdf)\n",
    "\n",
    "The following is DQN pseudocode\n",
    "\n",
    "![Pseudocode](https://lilianweng.github.io/lil-log/assets/images/DQN_algorithm.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DQN Network\n",
    "\n",
    "The DQN takes in the state input and outputs Q values for all possible actions.\n",
    "\n",
    "DQNs can only be used with discrete policies."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "        self.DQN = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions),\n",
    "                                        nn.LeakyReLU())\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.DQN(obs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replay Memory\n",
    "\n",
    "DQNs make use of experience replay memory - this stores previous transitions to allow for data efficient and non correlated learning to take place."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", (\"state\", \"action\", \"reward\", \"next_state\", \"done\"))\n",
    "\n",
    "class Replay_Memory:\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque([], maxlen=size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.memory):\n",
    "            return None\n",
    "        return random.sample(self.memory, batch_size)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent \n",
    "The following is the agent class:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "class DQN_Agent:\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,nb_actions,discount_factor=0.9,learning_rate=0.0001,epsilon_start=0.9,epsilon_end=0.05,epsilon_anneal_over_steps=10000,batch_size=256,update_frequency = 1000,replay_memory_size=100000):\n",
    "        \n",
    "        # Create the online network that the agent uses to select actions\n",
    "        self.online_net = DQN(input_size,hidden_size,nb_actions)\n",
    "\n",
    "        # Create the target network that the agent uses in it's updates\n",
    "        self.target_net = DQN(input_size,hidden_size,nb_actions)\n",
    "\n",
    "        self.num_actions = nb_actions\n",
    "\n",
    "        # Set the target net's initial weights to equal the online net\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        # User Huber Loss function - others can be use such as MSE\n",
    "        self.loss_function = torch.nn.SmoothL1Loss()\n",
    "\n",
    "        # DQN optimizer\n",
    "        self.optimizer = AdamW(self.online_net.parameters(),learning_rate)\n",
    "        \n",
    "        # Set initial and final epsilon values for epsilon greedy and choose the number of environment steps to anneal over\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_anneal_over_steps = epsilon_anneal_over_steps\n",
    "        self.step_no = 0\n",
    "\n",
    "        # Create Replay Memory and assign batch_size\n",
    "        self.replay = Replay_Memory(replay_memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Set update Frequency\n",
    "        self.update_frequency = update_frequency\n",
    "        \n",
    "        # Best Weights for Eval\n",
    "        self.best_weights = self.online_net.state_dict()\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        \"\"\"\n",
    "        Get current epsilon value according to agents total step number in the environment\n",
    "        \"\"\"\n",
    "        eps = self.epsilon_end\n",
    "        if self.step_no < self.epsilon_anneal_over_steps:\n",
    "            eps = self.epsilon_start - self.step_no * \\\n",
    "                ((self.epsilon_start - self.epsilon_end) /\n",
    "                 self.epsilon_anneal_over_steps)\n",
    "        return eps\n",
    "        \n",
    "    def act(self,obs):\n",
    "        # Increment global step count\n",
    "        self.step_no+=1\n",
    "\n",
    "        if np.random.uniform() > self.get_epsilon():\n",
    "            # Dont store gradients when acting in the environment\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.online_net(obs),dim=-1).view(1)\n",
    "        else:\n",
    "            return torch.tensor([random.randrange(self.num_actions)], dtype=torch.long)\n",
    "\n",
    "    # store experience in replay memory\n",
    "    def cache(self, state, action, reward, next_state, done):\n",
    "        self.replay.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def train(self):\n",
    "        self.online_net.train()\n",
    "        self.target_net.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.online_net.load_state_dict(self.best_weights)\n",
    "        self.online_net.eval()\n",
    "        self.target_net.eval()\n",
    "\n",
    "    def update_model(self):\n",
    "\n",
    "        # Get minibatch of data from experience buffer\n",
    "        batch = self.replay.sample(self.batch_size)\n",
    "\n",
    "        # If memory doesnt have enough transitions\n",
    "        if batch == None:\n",
    "            return\n",
    "\n",
    "        # Format batch to get a tensor of states, actions, rewards, next states and done booleans\n",
    "        batch_tuple = Transition(*zip(*batch))\n",
    "        state = torch.stack(batch_tuple.state)\n",
    "        action = torch.stack(batch_tuple.action)\n",
    "        reward = torch.stack(batch_tuple.reward)\n",
    "        next_state = torch.stack(batch_tuple.next_state)\n",
    "        done = torch.stack(batch_tuple.done)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Get the Q values of the online nets current state and actions\n",
    "        Q_Values = self.online_net(state).gather(1, action).squeeze()\n",
    "\n",
    "        # Get the max Q values of the target nets next state\n",
    "        Q_Targets = reward + (1 - done.float()) * self.gamma *self.target_net(next_state).max(dim=-1)[0].detach()\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.loss_function(Q_Values, Q_Targets)\n",
    "\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization step\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "\n",
    "    def update_target(self):\n",
    "        \"\"\"\n",
    "        Update the target nets weights\n",
    "        \"\"\"\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "    def update_best_weights(self):\n",
    "        self.best_weights = self.online_net.state_dict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "The last thing needed is the training loop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "from numpy import double\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "def train(agent, env, num_episodes, num_steps, log_every=20, render_from=1000):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to train.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    :param render_from: The episode number to start rendering to screen to allow one to view agent. Rendering significantly slows down training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    best_running_avg = 0\n",
    "    agent.train()\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = torch.tensor(env.reset(),dtype=torch.float32)\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "            if episode>render_from:\n",
    "                env.render()\n",
    "\n",
    "            # return chosen action\n",
    "            action = agent.act(obs)\n",
    "            \n",
    "            # Take a step in the environment with the action drawn\n",
    "            next_obs , reward, done, info = env.step(action.item())\n",
    "            \n",
    "            # Just for logging\n",
    "            reward_total+=reward\n",
    "\n",
    "            # change next state into tensor for update\n",
    "            next_obs = torch.tensor(next_obs,dtype=torch.float32)\n",
    "            \n",
    "            # change reward into tensor for update\n",
    "            reward = torch.tensor(\n",
    "                reward, dtype=torch.float32)\n",
    "\n",
    "            # Store transition in replay memory\n",
    "            agent.cache(obs,action,reward,next_obs,torch.tensor(done))\n",
    "\n",
    "            # Perform update\n",
    "            agent.update_model()\n",
    "            \n",
    "            # If the number of steps has elapsed then perform update\n",
    "            if agent.step_no % agent.update_frequency == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "            # Set the current state to the next state\n",
    "            obs = next_obs\n",
    "            \n",
    "            # if done then log and break\n",
    "            if done:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    if running_avg > best_running_avg:\n",
    "                        agent.update_best_weights()\n",
    "                        \n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "        \n",
    "\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "\n",
    "def eval(agent, env, num_episodes, num_steps, log_every=20):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to eval.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = torch.tensor(env.reset(),dtype=torch.float32)\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "           \n",
    "            env.render()\n",
    "            # return chosen action\n",
    "            action = agent.act(obs)\n",
    "            \n",
    "            # Take a step in the environment with the action drawn\n",
    "            next_obs , reward, done, info = env.step(action.item())\n",
    "            \n",
    "            # Just for logging\n",
    "            reward_total+=reward\n",
    "\n",
    "            # change next state into tensor for update\n",
    "            next_obs = torch.tensor(next_obs,dtype=torch.float32)\n",
    "            \n",
    "            # Set the current state to the next state\n",
    "            obs = next_obs\n",
    "           \n",
    "            # if done then log and break\n",
    "            if done:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time to Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Hyper parameters\n",
    "HIDDEN_SIZE = 512\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0005\n",
    "EPISODES_TO_TRAIN = 700\n",
    "EPISODES_TO_EVAL = 20\n",
    "MAX_STEPS = 201\n",
    "LOG_EVERY = 20\n",
    "RENDER_FROM_EP = 2000\n",
    "EPSILON_START=0.9\n",
    "EPSILON_END=0.01\n",
    "EPSILON_ANNEAL_OVER_STEPS=10000\n",
    "BATCH_SIZE=512\n",
    "UPDATE_FREQUENCY = 1500\n",
    "REPLAY_MEMORY_SIZE=500000\n",
    "\n",
    "\n",
    "agent = DQN_Agent(env.observation_space.shape[0],HIDDEN_SIZE,nb_actions,GAMMA,LEARNING_RATE,EPSILON_START,EPSILON_END,EPSILON_ANNEAL_OVER_STEPS,BATCH_SIZE,UPDATE_FREQUENCY,REPLAY_MEMORY_SIZE)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "train(agent,env,EPISODES_TO_TRAIN,MAX_STEPS,LOG_EVERY,RENDER_FROM_EP)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "\n",
    "eval(agent,env,EPISODES_TO_EVAL,MAX_STEPS,log_every=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training...\n",
      "Episode   20 finished after   28 timesteps with a total reward of 28.0 | Running Average: 19.74\n",
      "Episode   40 finished after   19 timesteps with a total reward of 19.0 | Running Average: 22.18\n",
      "Episode   60 finished after   12 timesteps with a total reward of 12.0 | Running Average: 21.03\n",
      "Episode   80 finished after   21 timesteps with a total reward of 21.0 | Running Average: 20.77\n",
      "Episode  100 finished after   35 timesteps with a total reward of 35.0 | Running Average: 20.73\n",
      "Episode  120 finished after   30 timesteps with a total reward of 30.0 | Running Average: 22.35\n",
      "Episode  140 finished after   87 timesteps with a total reward of 87.0 | Running Average: 22.76\n",
      "Episode  160 finished after   16 timesteps with a total reward of 16.0 | Running Average: 25.82\n",
      "Episode  180 finished after   38 timesteps with a total reward of 38.0 | Running Average: 27.85\n",
      "Episode  200 finished after   18 timesteps with a total reward of 18.0 | Running Average: 29.04\n",
      "Episode  220 finished after  117 timesteps with a total reward of 117.0 | Running Average: 31.76\n",
      "Episode  240 finished after  200 timesteps with a total reward of 200.0 | Running Average: 41.97\n",
      "Episode  260 finished after  136 timesteps with a total reward of 136.0 | Running Average: 64.12\n",
      "Episode  280 finished after  124 timesteps with a total reward of 124.0 | Running Average: 83.37\n",
      "Episode  300 finished after  136 timesteps with a total reward of 136.0 | Running Average: 103.31\n",
      "Episode  320 finished after  120 timesteps with a total reward of 120.0 | Running Average: 115.90\n",
      "Episode  340 finished after  163 timesteps with a total reward of 163.0 | Running Average: 123.02\n",
      "Episode  360 finished after  119 timesteps with a total reward of 119.0 | Running Average: 116.88\n",
      "Episode  380 finished after  103 timesteps with a total reward of 103.0 | Running Average: 111.45\n",
      "Episode  400 finished after  100 timesteps with a total reward of 100.0 | Running Average: 106.50\n",
      "Episode  420 finished after  104 timesteps with a total reward of 104.0 | Running Average: 100.46\n",
      "Episode  440 finished after  102 timesteps with a total reward of 102.0 | Running Average: 96.39\n",
      "Episode  460 finished after  200 timesteps with a total reward of 200.0 | Running Average: 91.42\n",
      "Episode  480 finished after  131 timesteps with a total reward of 131.0 | Running Average: 107.07\n",
      "Episode  500 finished after  119 timesteps with a total reward of 119.0 | Running Average: 116.40\n",
      "Episode  520 finished after  138 timesteps with a total reward of 138.0 | Running Average: 134.67\n",
      "Episode  540 finished after  117 timesteps with a total reward of 117.0 | Running Average: 138.75\n",
      "Episode  560 finished after  101 timesteps with a total reward of 101.0 | Running Average: 141.33\n",
      "Episode  580 finished after  116 timesteps with a total reward of 116.0 | Running Average: 127.52\n",
      "Episode  600 finished after  152 timesteps with a total reward of 152.0 | Running Average: 126.33\n",
      "Episode  620 finished after  200 timesteps with a total reward of 200.0 | Running Average: 116.75\n",
      "Episode  640 finished after   91 timesteps with a total reward of 91.0 | Running Average: 119.87\n",
      "Episode  660 finished after   94 timesteps with a total reward of 94.0 | Running Average: 119.22\n",
      "Episode  680 finished after   96 timesteps with a total reward of 96.0 | Running Average: 112.42\n",
      "Episode  700 finished after  106 timesteps with a total reward of 106.0 | Running Average: 101.69\n",
      "Evaluating...\n",
      "Episode    2 finished after  102 timesteps with a total reward of 102.0 | Running Average: 105.00\n",
      "Episode    3 finished after  107 timesteps with a total reward of 107.0 | Running Average: 103.50\n",
      "Episode    4 finished after   12 timesteps with a total reward of 12.0 | Running Average: 104.67\n",
      "Episode    5 finished after  101 timesteps with a total reward of 101.0 | Running Average: 81.50\n",
      "Episode    6 finished after   18 timesteps with a total reward of 18.0 | Running Average: 85.40\n",
      "Episode    7 finished after   26 timesteps with a total reward of 26.0 | Running Average: 74.17\n",
      "Episode    8 finished after  110 timesteps with a total reward of 110.0 | Running Average: 67.29\n",
      "Episode    9 finished after  104 timesteps with a total reward of 104.0 | Running Average: 72.62\n",
      "Episode   10 finished after  112 timesteps with a total reward of 112.0 | Running Average: 76.11\n",
      "Episode   11 finished after  109 timesteps with a total reward of 109.0 | Running Average: 79.70\n",
      "Episode   12 finished after   92 timesteps with a total reward of 92.0 | Running Average: 82.36\n",
      "Episode   13 finished after  108 timesteps with a total reward of 108.0 | Running Average: 83.17\n",
      "Episode   14 finished after   10 timesteps with a total reward of 10.0 | Running Average: 85.08\n",
      "Episode   15 finished after  112 timesteps with a total reward of 112.0 | Running Average: 79.71\n",
      "Episode   16 finished after  106 timesteps with a total reward of 106.0 | Running Average: 81.87\n",
      "Episode   17 finished after   65 timesteps with a total reward of 65.0 | Running Average: 83.38\n",
      "Episode   18 finished after   92 timesteps with a total reward of 92.0 | Running Average: 82.29\n",
      "Episode   19 finished after   28 timesteps with a total reward of 28.0 | Running Average: 82.83\n",
      "Episode   20 finished after  108 timesteps with a total reward of 108.0 | Running Average: 79.95\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "05a25e9113b15a9f940c6517e4f0bb1aaafcc84783dc3502a8f30808da99fb6f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}