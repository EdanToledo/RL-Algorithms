{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DQN\n",
    "\n",
    "DQN (Deep Q-Network) is a method to implement Q learning - an off policy value based method - using deep neural networks. Q learning when using non linear function approximators can be very unstable and data inefficient so DQN addresses these problems. [DQN Paper](https://arxiv.org/pdf/1312.5602.pdf)\n",
    "\n",
    "The following is DQN pseudocode\n",
    "\n",
    "![Pseudocode](https://lilianweng.github.io/lil-log/assets/images/DQN_algorithm.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DQN Network\n",
    "\n",
    "The DQN takes in the state input and outputs Q values for all possible actions.\n",
    "\n",
    "DQNs can only be used with discrete policies."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "        self.DQN = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions),\n",
    "                                        nn.LeakyReLU())\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.DQN(obs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replay Memory\n",
    "\n",
    "DQNs make use of experience replay memory - this stores previous transitions to allow for data efficient and non correlated learning to take place."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", (\"state\", \"action\", \"reward\", \"next_state\", \"done\"))\n",
    "\n",
    "class Replay_Memory:\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque([], maxlen=size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.memory):\n",
    "            return None\n",
    "        return random.sample(self.memory, batch_size)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent \n",
    "The following is the agent class:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "class DQN_Agent:\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,nb_actions,discount_factor=0.9,learning_rate=0.0001,epsilon_start=0.9,epsilon_end=0.05,epsilon_anneal_over_steps=10000,batch_size=256,update_frequency = 1000,replay_memory_size=100000):\n",
    "        \n",
    "        # Create the online network that the agent uses to select actions\n",
    "        self.online_net = DQN(input_size,hidden_size,nb_actions)\n",
    "\n",
    "        # Create the target network that the agent uses in it's updates\n",
    "        self.target_net = DQN(input_size,hidden_size,nb_actions)\n",
    "\n",
    "        self.num_actions = nb_actions\n",
    "\n",
    "        # Set the target net's initial weights to equal the online net\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        # User Huber Loss function - others can be use such as MSE\n",
    "        self.loss_function = torch.nn.SmoothL1Loss()\n",
    "\n",
    "        # DQN optimizer\n",
    "        self.optimizer = AdamW(self.online_net.parameters(),learning_rate)\n",
    "        \n",
    "        # Set initial and final epsilon values for epsilon greedy and choose the number of environment steps to anneal over\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_anneal_over_steps = epsilon_anneal_over_steps\n",
    "        self.step_no = 0\n",
    "\n",
    "        # Create Replay Memory and assign batch_size\n",
    "        self.replay = Replay_Memory(replay_memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Set update Frequency\n",
    "        self.update_frequency = update_frequency\n",
    "        \n",
    "        # Best Weights for Eval\n",
    "        self.best_weights = self.online_net.state_dict()\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        \"\"\"\n",
    "        Get current epsilon value according to agents total step number in the environment\n",
    "        \"\"\"\n",
    "        eps = self.epsilon_end\n",
    "        if self.step_no < self.epsilon_anneal_over_steps:\n",
    "            eps = self.epsilon_start - self.step_no * \\\n",
    "                ((self.epsilon_start - self.epsilon_end) /\n",
    "                 self.epsilon_anneal_over_steps)\n",
    "        return eps\n",
    "        \n",
    "    def act(self,obs):\n",
    "        # Increment global step count\n",
    "        self.step_no+=1\n",
    "\n",
    "        if np.random.uniform() > self.get_epsilon():\n",
    "            # Dont store gradients when acting in the environment\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.online_net(obs),dim=-1).view(1)\n",
    "        else:\n",
    "            return torch.tensor([random.randrange(self.num_actions)], dtype=torch.long)\n",
    "\n",
    "    # store experience in replay memory\n",
    "    def cache(self, state, action, reward, next_state, done):\n",
    "        self.replay.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def train(self):\n",
    "        self.online_net.train()\n",
    "        self.target_net.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.online_net.load_state_dict(self.best_weights)\n",
    "        self.online_net.eval()\n",
    "        self.target_net.eval()\n",
    "\n",
    "    def update_model(self):\n",
    "\n",
    "        # Get minibatch of data from experience buffer\n",
    "        batch = self.replay.sample(self.batch_size)\n",
    "\n",
    "        # If memory doesnt have enough transitions\n",
    "        if batch == None:\n",
    "            return\n",
    "\n",
    "        # Format batch to get a tensor of states, actions, rewards, next states and done booleans\n",
    "        batch_tuple = Transition(*zip(*batch))\n",
    "        state = torch.stack(batch_tuple.state)\n",
    "        action = torch.stack(batch_tuple.action)\n",
    "        reward = torch.stack(batch_tuple.reward)\n",
    "        next_state = torch.stack(batch_tuple.next_state)\n",
    "        done = torch.stack(batch_tuple.done)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Get the Q values of the online nets current state and actions\n",
    "        Q_Values = self.online_net(state).gather(1, action).squeeze()\n",
    "\n",
    "        # Get the max Q values of the target nets next state\n",
    "        Q_Targets = reward + (1 - done.float()) * self.gamma * self.target_net(next_state).max(dim=-1)[0].detach()\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.loss_function(Q_Values, Q_Targets)\n",
    "\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization step\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "\n",
    "    def update_target(self):\n",
    "        \"\"\"\n",
    "        Update the target nets weights\n",
    "        \"\"\"\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "    def update_best_weights(self):\n",
    "        self.best_weights = self.online_net.state_dict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "The last thing needed is the training loop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "from numpy import double\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "def train(agent, env, num_episodes, num_steps, log_every=20, render_from=1000):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to train.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    :param render_from: The episode number to start rendering to screen to allow one to view agent. Rendering significantly slows down training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    best_running_avg = 0\n",
    "    agent.train()\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = torch.tensor(env.reset(),dtype=torch.float32)\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "            if episode>render_from:\n",
    "                env.render()\n",
    "\n",
    "            # return chosen action\n",
    "            action = agent.act(obs)\n",
    "            \n",
    "            # Take a step in the environment with the action drawn\n",
    "            next_obs , reward, done, info = env.step(action.item())\n",
    "            \n",
    "            # Just for logging\n",
    "            reward_total+=reward\n",
    "\n",
    "            # change next state into tensor for update\n",
    "            next_obs = torch.tensor(next_obs,dtype=torch.float32)\n",
    "            \n",
    "            # change reward into tensor for update\n",
    "            reward = torch.tensor(\n",
    "                reward, dtype=torch.float32)\n",
    "\n",
    "            # Store transition in replay memory\n",
    "            agent.cache(obs,action,reward,next_obs,torch.tensor(done))\n",
    "\n",
    "            # Perform update\n",
    "            agent.update_model()\n",
    "            \n",
    "            # If the number of steps has elapsed then perform update\n",
    "            if agent.step_no % agent.update_frequency == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "            # Set the current state to the next state\n",
    "            obs = next_obs\n",
    "            \n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    if running_avg > best_running_avg:\n",
    "                        best_running_avg = running_avg\n",
    "                        agent.update_best_weights()\n",
    "                        \n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "        \n",
    "\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def eval(agent, env, num_episodes, num_steps, log_every=20):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to eval.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = torch.tensor(env.reset(),dtype=torch.float32)\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "           \n",
    "            env.render()\n",
    "            # return chosen action\n",
    "            action = agent.act(obs)\n",
    "            \n",
    "            # Take a step in the environment with the action drawn\n",
    "            next_obs , reward, done, info = env.step(action.item())\n",
    "            \n",
    "            # Just for logging\n",
    "            reward_total+=reward\n",
    "\n",
    "            # change next state into tensor for update\n",
    "            next_obs = torch.tensor(next_obs,dtype=torch.float32)\n",
    "            \n",
    "            # Set the current state to the next state\n",
    "            obs = next_obs\n",
    "           \n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time to Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Hyper parameters\n",
    "HIDDEN_SIZE = 512\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0005\n",
    "EPISODES_TO_TRAIN = 700\n",
    "EPISODES_TO_EVAL = 20\n",
    "MAX_STEPS = 201\n",
    "LOG_EVERY = 20\n",
    "RENDER_FROM_EP = 2000\n",
    "EPSILON_START=0.9\n",
    "EPSILON_END=0.01\n",
    "EPSILON_ANNEAL_OVER_STEPS=10000\n",
    "BATCH_SIZE=512\n",
    "UPDATE_FREQUENCY = 1500\n",
    "REPLAY_MEMORY_SIZE=500000\n",
    "\n",
    "\n",
    "agent = DQN_Agent(env.observation_space.shape[0],HIDDEN_SIZE,nb_actions,GAMMA,LEARNING_RATE,EPSILON_START,EPSILON_END,EPSILON_ANNEAL_OVER_STEPS,BATCH_SIZE,UPDATE_FREQUENCY,REPLAY_MEMORY_SIZE)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "train(agent,env,EPISODES_TO_TRAIN,MAX_STEPS,LOG_EVERY,RENDER_FROM_EP)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "\n",
    "eval(agent,env,EPISODES_TO_EVAL,MAX_STEPS,log_every=1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "05a25e9113b15a9f940c6517e4f0bb1aaafcc84783dc3502a8f30808da99fb6f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}