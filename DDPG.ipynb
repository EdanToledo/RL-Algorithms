{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DDPG - Deep Deterministic Policy Gradient\n",
    "DDPG is the deep reinforcement learning implementation of DPG - Deterministic Policy Gradient - that employs the methods learnt from DQN to DPG. The DPG algorithm is a policy gradient method that uses a determinisitc policy instead of a stochastic one. This created the question of how to calculate the gradient of the action if it has no probability to which it was shown to be quite simple and easy. DPG offers more efficient gradient estimation than classical policy gradient methods and ,in practice, has been shown to outperform stochastic counterparts. DPG is regarded as an off policy actor-critic method that utilises stochastic/exploratory policies to learn a deterministic target policy.\n",
    "\n",
    "The Deterministic Policy Gradient is as follows:\n",
    "$\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta) \n",
    "&= \\int_\\mathcal{S} \\rho^\\mu(s) \\nabla_a Q^\\mu(s, a) \\nabla_\\theta \\mu_\\theta(s) \\rvert_{a=\\mu_\\theta(s)} ds \\\\\n",
    "&= \\mathbb{E}_{s \\sim \\rho^\\mu} [\\nabla_a Q^\\mu(s, a) \\nabla_\\theta \\mu_\\theta(s) \\rvert_{a=\\mu_\\theta(s)}]\n",
    "\\end{aligned}$\n",
    "\n",
    "DDPG is essentially a combination of DPG and DQN where DDPG makes DQN able to work in continuous environments. \n",
    "\n",
    "The following is the DDPG Pseudocode:\n",
    "![Pseudocode](https://lilianweng.github.io/lil-log/assets/images/DDPG_algo.png)\n",
    "\n",
    "Since the policy is deterministic, noise is added to the behaviour policy to aid exploration."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Continuous Policy\n",
    "The continuous action space policy for DPG is deterministic which means we only need one neural network to approximate the action value.\n",
    "\n",
    "The sizes the neural network is as follows:\n",
    "\n",
    "- Input : State Observation size\n",
    "- Hidden : Whatever you please.\n",
    "- Output: 1\n",
    "\n",
    "Batch normalisation really helps with different dimensional spaces."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Continuous_Policy(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, nb_actions) -> None:\n",
    "        super(Continuous_Policy, self).__init__()\n",
    "        self.mean_net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.BatchNorm1d(hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, nb_actions)\n",
    "                                        )\n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.mean_net(obs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Critic\n",
    "This is the critic model - a neural network to learn the value function. This could be a Q (action-value) or V (state-value) approximator.  In this case it is the Q value we are approximating. For this I simply concatenate the state observation and action since the action is deterministic."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size,nb_actions, hidden_size) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        self.critic = nn.Sequential(nn.Linear(input_size+nb_actions, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(hidden_size, 1))\n",
    "        \n",
    "    def forward(self,obs,action):\n",
    "        # Concatenate the action and state values\n",
    "        return self.critic(torch.cat([obs,action],dim=-1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replay Memory\n",
    "\n",
    "Just like DQNs, DDPG makes use of experience replay memory - this stores previous transitions to allow for data efficient and non correlated learning to take place."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", (\"state\", \"action\", \"reward\", \"next_state\", \"done\"))\n",
    "\n",
    "class Replay_Memory:\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque([], maxlen=size)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.memory):\n",
    "            return None\n",
    "        return random.sample(self.memory, batch_size)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent \n",
    "The following is the agent class:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "\n",
    "class DDPG_Agent:\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,nb_actions,discount_factor=0.9,learning_rate=0.0001,batch_size=256,replay_memory_size=100000,polyak = 0.005,noise_dev=0.1):\n",
    "        \n",
    "        # Create the online network critic\n",
    "        self.online_net = Critic(input_size,nb_actions,hidden_size)\n",
    "\n",
    "        # Create the target network critic\n",
    "        self.target_net = Critic(input_size,nb_actions,hidden_size)\n",
    "\n",
    "        # Create the online policy\n",
    "        self.online_policy = Continuous_Policy(input_size,hidden_size,nb_actions)\n",
    "\n",
    "        # Create the target policy\n",
    "        self.target_policy = Continuous_Policy(input_size,hidden_size,nb_actions)\n",
    "       \n",
    "        # Polyak averaging weight\n",
    "        self.polyak = polyak\n",
    "        # Gaussian Noise for exploratory behaviour in training\n",
    "        self.noise = Normal(torch.zeros(nb_actions),noise_dev)\n",
    "\n",
    "        # Set the targets initial weights to equal the online weights\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        self.target_policy.load_state_dict(self.online_policy.state_dict())\n",
    "\n",
    "        # Future reward discount factor\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "        # User MSE Loss function\n",
    "        self.loss_function = torch.nn.MSELoss()\n",
    "\n",
    "        # Critic and policy optimizer\n",
    "        self.critic_optimizer = AdamW(self.online_net.parameters(),learning_rate)\n",
    "        self.policy_optimizer = AdamW(self.online_policy.parameters(),learning_rate)\n",
    "\n",
    "        # Create Replay Memory and assign batch_size\n",
    "        self.replay = Replay_Memory(replay_memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Best Weights for Eval\n",
    "        self.best_weights = self.online_policy.state_dict()\n",
    "        \n",
    "    def act(self,obs,with_noise):\n",
    "        # Put online policy into eval mode \n",
    "        self.online_policy.eval()\n",
    "        # Dont store gradients when acting in the environment\n",
    "        with torch.no_grad():\n",
    "            action = self.online_policy(obs)\n",
    "            # Add noise\n",
    "            if with_noise:\n",
    "                action += self.noise.sample()\n",
    "        # Put the online policy back into train mode\n",
    "        self.online_policy.train()\n",
    "        return action\n",
    "\n",
    "   # store experience in replay memory\n",
    "    def cache(self, state, action, reward, next_state, done):\n",
    "        self.replay.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def train(self):\n",
    "        self.online_policy.train()\n",
    "        self.target_policy.train()\n",
    "        self.online_net.train()\n",
    "        self.target_net.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.online_policy.load_state_dict(self.best_weights)\n",
    "        self.online_policy.eval()\n",
    "        self.target_policy.eval()\n",
    "        self.online_net.eval()\n",
    "        self.target_net.eval()\n",
    "        \n",
    "    def update_model(self):\n",
    "        \n",
    "        # Get minibatch of data from experience buffer\n",
    "        batch = self.replay.sample(self.batch_size)\n",
    "\n",
    "        # If memory doesnt have enough transitions\n",
    "        if batch == None:\n",
    "            return\n",
    "\n",
    "        # Format batch to get a tensor of states, actions, rewards, next states and done booleans\n",
    "        batch_tuple = Transition(*zip(*batch))\n",
    "        state = torch.cat(batch_tuple.state,dim=0)\n",
    "        action = torch.cat(batch_tuple.action,dim=0)\n",
    "        reward = torch.cat(batch_tuple.reward,dim=0)\n",
    "        next_state = torch.cat(batch_tuple.next_state,dim=0)\n",
    "        done = torch.cat(batch_tuple.done,dim=0)\n",
    "\n",
    "        # Freeze online net parameters as gradients should not be kept for policy loss\n",
    "        for param in self.online_net.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        policy_loss = -self.online_net(state,self.online_policy(state)).mean()\n",
    "\n",
    "        # Perform policy update\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        # Unfreeze online net parameters as gradients are needed\n",
    "        for param in self.online_net.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Get Q values current Q values for critic loss\n",
    "        Q_values = self.online_net(state,action).squeeze()\n",
    "\n",
    "        # Get Q Targets\n",
    "        Q_targets = reward + (1 - done.float()) * self.gamma*self.target_net(next_state,self.target_policy(next_state)).squeeze()\n",
    "\n",
    "        # Calculate critic loss\n",
    "        critic_loss = self.loss_function(Q_values, Q_targets.detach())\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        critic_loss.backward()\n",
    "    \n",
    "        # Perform critic update\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "    def update_best_weights(self):\n",
    "        self.best_weights = self.online_policy.state_dict()\n",
    "\n",
    "    def update_target(self):\n",
    "        \"\"\"\n",
    "        Use polyak averaging to update weights\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for online_policy_param, target_policy_param in zip(self.online_policy.parameters(), self.target_policy.parameters()):\n",
    "                target_policy_param.data.mul_(1-self.polyak)\n",
    "                target_policy_param.data.add_(self.polyak* online_policy_param.data)\n",
    "            \n",
    "            for online_net_param, target_net_param in zip(self.online_net.parameters(), self.target_net.parameters()):\n",
    "                target_net_param.data.mul_(1-self.polyak)\n",
    "                target_net_param.data.add_(self.polyak* online_net_param.data)\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "The last thing needed is the training loop."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import deque\n",
    "import gym\n",
    "from numpy import double\n",
    "import torch\n",
    "import argparse\n",
    "from torch.distributions import Normal\n",
    "\n",
    "def train(agent, env, num_episodes, num_steps, log_every=20, render_from=1000):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to train.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    :param render_from: The episode number to start rendering to screen to allow one to view agent. Rendering significantly slows down training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    best_running_avg = 0\n",
    "    agent.train()\n",
    "\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation - unsqueeze to give batch dimension of 1\n",
    "        obs = torch.tensor(env.reset(),dtype=torch.float32).unsqueeze(0)\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "            if episode>render_from:\n",
    "                env.render()\n",
    "\n",
    "            # return chosen action\n",
    "            action = agent.act(obs,with_noise=True)\n",
    "            \n",
    "            # Take a step in the environment with the action drawn\n",
    "            next_obs , reward, done, info = env.step(action.squeeze().numpy())\n",
    "            \n",
    "            # Just for logging\n",
    "            reward_total+=reward\n",
    "\n",
    "            # change next state into tensor for update - give batch dimension of 1\n",
    "            next_obs = torch.tensor(next_obs,dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "            # change reward into tensor for update - give batch dimension of 1\n",
    "            reward = torch.tensor(\n",
    "                reward, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            # Store transition in replay memory\n",
    "            agent.cache(obs,action,reward,next_obs,torch.tensor(done).unsqueeze(0))\n",
    "\n",
    "            # Perform update\n",
    "            agent.update_model()\n",
    "            \n",
    "            # Update target networks\n",
    "            agent.update_target()\n",
    "\n",
    "            # Set the current state to the next state\n",
    "            obs = next_obs\n",
    "            \n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    if running_avg > best_running_avg:\n",
    "                        best_running_avg = running_avg\n",
    "                        agent.update_best_weights()\n",
    "                        \n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "        \n",
    "\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def eval(agent, env, num_episodes, num_steps, log_every=20):\n",
    "    \"\"\"\n",
    "    :param agent: the agent to be trained.\n",
    "    :param env: the gym environment.\n",
    "    :param num_episodes: the number of episodes to eval.\n",
    "    :param num_steps: the max number of steps possible per episode.\n",
    "    :param log_every: The frequency of logging. Default logs every 20 episodes.\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    # Running Average Reward Memory\n",
    "    running_avg_reward = deque(maxlen=100)\n",
    "    for episode in range(1,num_episodes+1):\n",
    "        # Starting state observation\n",
    "        obs = torch.tensor(env.reset(),dtype=torch.float32).unsqueeze(0)\n",
    "        reward_total = 0\n",
    "        for step in range(num_steps):\n",
    "           \n",
    "            env.render()\n",
    "            # return chosen action\n",
    "            action = agent.act(obs,with_noise=False)\n",
    "            \n",
    "            # Take a step in the environment with the action drawn\n",
    "            next_obs , reward, done, info = env.step(action.numpy())\n",
    "            \n",
    "            # Just for logging\n",
    "            reward_total+=reward\n",
    "\n",
    "            # change next state into tensor for update\n",
    "            next_obs = torch.tensor(next_obs,dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "            # Set the current state to the next state\n",
    "            obs = next_obs\n",
    "           \n",
    "            # if done then log and break\n",
    "            if done or step == num_steps-1:\n",
    "                if episode % log_every ==0 and len(running_avg_reward)>0:\n",
    "                    running_avg = sum(running_avg_reward)/len(running_avg_reward)\n",
    "                    print(\"Episode {0:4d} finished after {1:4d} timesteps with a total reward of {2:3.1f} | Running Average: {3:3.2f}\".format(episode,step+1,reward_total,running_avg))\n",
    "                break\n",
    "        running_avg_reward.append(reward_total)\n",
    "    env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time to Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Hyper parameters\n",
    "HIDDEN_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "EPISODES_TO_TRAIN = 1000\n",
    "EPISODES_TO_EVAL = 20\n",
    "MAX_STEPS = 200\n",
    "LOG_EVERY = 20\n",
    "RENDER_FROM_EP = 2000\n",
    "BATCH_SIZE=100\n",
    "REPLAY_MEMORY_SIZE=1000000\n",
    "POLYAK = 0.005\n",
    "NOISE_STD_DEV = 0.1\n",
    "\n",
    "agent = DDPG_Agent(env.observation_space.shape[0],HIDDEN_SIZE,nb_actions,GAMMA,LEARNING_RATE,BATCH_SIZE,REPLAY_MEMORY_SIZE,POLYAK,NOISE_STD_DEV)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "train(agent,env,EPISODES_TO_TRAIN,MAX_STEPS,LOG_EVERY,RENDER_FROM_EP)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "\n",
    "eval(agent,env,EPISODES_TO_EVAL,MAX_STEPS,log_every=1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "05a25e9113b15a9f940c6517e4f0bb1aaafcc84783dc3502a8f30808da99fb6f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}